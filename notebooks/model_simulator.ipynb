{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaI6TYNTUxLc",
    "outputId": "2cbb1abb-933d-4d4a-d5f7-0488e28d65f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTAÇÕES\n",
    "# ==========================================\n",
    "\n",
    "# Bibliotecas para manipulação do sistema e ambiente de aprendizado por reforço\n",
    "import os\n",
    "\n",
    "# Bibliotecas para operações matemáticas e aleatoriedade\n",
    "import numpy as np\n",
    "import random\n",
    "import logging  # Para registro de mensagens e depuração\n",
    "import itertools  # Para geração de combinações e permutações\n",
    "import statistics\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "\n",
    "\n",
    "# Biblioteca para manipulações de DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca para operações de Deep Learning com TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, InputLayer, MultiHeadAttention, LayerNormalization, Reshape, MultiHeadAttention, GlobalAveragePooling2D, TimeDistributed, MaxPooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam  # Otimizador para ajuste dos pesos da rede neural\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber\n",
    "from tensorflow.keras.regularizers import l2  # Regularização L2 para controle de overfitting\n",
    "from tensorflow.keras.models import load_model, Model  # Carregar modelos pré-treinados, \n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import (InputLayer, Conv2D, BatchNormalization, Flatten, \n",
    "                                     Dense, Dropout, Activation, Concatenate, Multiply, Layer)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Verificar dispositivos físicos disponíveis (por exemplo, GPU)\n",
    "#device_lib.list_local_devices()\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TWku-2ZkUxLe"
   },
   "outputs": [],
   "source": [
    "#Class: Agents\n",
    "# ==========================================\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, x, y, env, breed, channel):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.env = env\n",
    "        self.breed = breed\n",
    "        self.channel = channel\n",
    "        self.is_alive = True\n",
    "        self.is_done = False\n",
    "        self.current_target = None\n",
    "        self.current_ally = None\n",
    "        self.last_coord = self.x, self.y\n",
    "        self.kid = False\n",
    "\n",
    "        # Category         | Breed | Color  |     Channel      | Color - Att | Channel - Att\n",
    "        # ---------------------------------------------------------------------------\n",
    "        # Predator         |   0   | Red    |    [1, 0, 0]     |   Yellow     | [1, 1, 0]\n",
    "        # Vegetation       |   1   | Green  |    [0, 1, 0]     |  --------    | ---------\n",
    "        # Prey             |   2   | Blue   |    [0, 0, 1]     |   Cyan       | [0, 1, 1]\n",
    "        # Outside Grid     |   -   | White  |    [1, 1, 1]     |  --------    | ---------\n",
    "        # Empty Grid Cell  |   -   | Black  |    [0, 0, 0]     |  --------    | ---------\n",
    "        # Reserved Training|   -   | Gray   | [0.5, 0.5, 0.5]  |  --------    | ---------\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.x, self.y = env.new_position()\n",
    "        self.is_alive = True\n",
    "        self.target = None\n",
    "        self.is_done = False\n",
    "        self.current_target = None\n",
    "        self.current_ally = None\n",
    "        \n",
    "        # Restaurar o `channel` para a cor padrão com base no `breed`\n",
    "        if self.breed == 0:  # Predador\n",
    "            self.channel = [1.0, 0.0, 0.0]\n",
    "        elif self.breed == 2:  # Presa\n",
    "            self.channel = [0.0, 0.0, 1.0]\n",
    "\n",
    "    def step(self, action):\n",
    "        penalty = self.env.move_agent(self, action)\n",
    "        done, reward, ler = self.check_goal()\n",
    "        state = self.env.render_agent(self)\n",
    "\n",
    "        return state, reward + penalty, done, ler\n",
    "    \n",
    "class Prey(Agent):\n",
    "    def __init__(self, x, y, env, id, nn = None, is_on = True):\n",
    "        super().__init__(x, y, env, breed=2, channel=[0.0, 0.0, 1.0])\n",
    "        self.name = f\"Prey_{id}\"\n",
    "        self.in_danger = False\n",
    "        self.is_on = is_on\n",
    "        self.model_nn = nn\n",
    "\n",
    "    def check_goal(self):\n",
    "        done = False\n",
    "        reward = 5.0  # Recompensa padrão para incentivar a exploração segura.\n",
    "        feedback = \"\"\n",
    "        target_detected = False\n",
    "        ally_detected = False\n",
    "\n",
    "        closest_target_distance = float('inf')\n",
    "        closest_ally_distance = float('inf')\n",
    "\n",
    "        # Procura por agente vivo, presa ou predador, mais próxima ou mantém o foco na atual.\n",
    "        for agent in self.env.agents:\n",
    "            # Verifica se é predador\n",
    "            if agent.breed == 0 and agent.is_alive:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_target is None or distance < closest_target_distance):\n",
    "                    closest_target_distance = distance\n",
    "                    self.current_target = agent\n",
    "                    target_detected = True\n",
    "\n",
    "            # Verifica se é presa com informação de predador\n",
    "            elif agent.breed == 2 and agent.is_alive and agent.channel == [0, 1, 1]:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_ally is None or distance < closest_ally_distance):\n",
    "                    closest_ally_distance = distance\n",
    "                    self.current_ally = agent\n",
    "                    ally_detected = True\n",
    "\n",
    "        \n",
    "        # Verificação de fuga do predador\n",
    "        if self.in_danger and closest_target_distance > 3:\n",
    "            done = True\n",
    "            self.in_danger = False\n",
    "            reward += 20.0  # Recompensa máxima por escapar do perigo\n",
    "            feedback = f\"[PREY]: Evasao do alvo\"\n",
    "        \n",
    "        # Critérios de Recompensa/Penalidade\n",
    "        elif target_detected and 0 < closest_target_distance <= 3:\n",
    "            # Penalidade por proximidade com predador\n",
    "            self.in_danger = True\n",
    "            if closest_target_distance == 3:\n",
    "                reward -= 1.0\n",
    "            elif closest_target_distance == 2:\n",
    "                reward -= 3.0\n",
    "            elif closest_target_distance == 1:\n",
    "                reward -= 5.0\n",
    "            feedback = f\"[PREY]: Proximidade predador: {closest_target_distance}\"\n",
    "\n",
    "        elif ally_detected and not target_detected and 0 < closest_ally_distance <= 3:\n",
    "            # Recompensa por proximidade com aliado (se não houver predador no campo de visão)\n",
    "            if closest_ally_distance == 3:\n",
    "                reward -= 0.1\n",
    "            elif closest_ally_distance == 2:\n",
    "                reward -= 0.3\n",
    "            elif closest_ally_distance == 1:\n",
    "                reward -= 0.5\n",
    "            feedback += f\"[PREY]: Proximidade aliado: {closest_ally_distance}\"\n",
    "\n",
    "        else:\n",
    "            feedback += \"[PREY]: Explorando mapa\"\n",
    "\n",
    "\n",
    "        # Atualiza channel\n",
    "        for agent in self.env.agents:\n",
    "            if agent.is_alive and agent.breed == 0:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray:\n",
    "                    self.channel = [0.0, 1.0, 1.0]\n",
    "                    break\n",
    "                else:\n",
    "                    self.channel = [0.0, 0.0, 1.0]\n",
    "\n",
    "        return done, reward, feedback\n",
    "\n",
    "class Predator(Agent):\n",
    "    def __init__(self, x, y, env, id, nn = None, is_on = True):\n",
    "        super().__init__(x, y, env, breed=0, channel=[1.0, 0.0, 0.0])\n",
    "        self.name = f\"Predador_{id}\"\n",
    "        self.last_hunt = None\n",
    "        self.is_on = is_on\n",
    "\n",
    "    def check_goal(self):\n",
    "        done = False\n",
    "        reward = -0.05  # Penalidade leve para incentivar movimentação\n",
    "        feedback = \"\"\n",
    "        target_detected = False\n",
    "        ally_detected = False\n",
    "\n",
    "        closest_target_distance = float('inf')\n",
    "        closest_ally_distance = float('inf')\n",
    "\n",
    "        # Procura por agente vivo, presa ou predador, mais próxima ou mantém o foco na atual.\n",
    "        for agent in self.env.agents:\n",
    "            # Verifica se é presa\n",
    "            if agent.is_alive and agent.breed == 2:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_target is None or distance < closest_target_distance):\n",
    "                    closest_target_distance = distance\n",
    "                    self.current_target = agent\n",
    "                    target_detected = True\n",
    "\n",
    "            # Verifica se é predador com informação de presa\n",
    "            elif agent.breed == 0 and agent.is_alive and agent.channel == [1.0, 1.0, 0.0]:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_ally is None or distance < closest_ally_distance):\n",
    "                    closest_ally_distance = distance\n",
    "                    self.current_ally = agent\n",
    "                    ally_detected = True\n",
    "\n",
    "        # Critérios de Recompensa\n",
    "        if target_detected and closest_target_distance == 0:\n",
    "            done = True\n",
    "            reward += 20.0  # Recompensa máxima por capturar a presa\n",
    "            self.current_target.is_alive = False\n",
    "            feedback = f\"[PREDATOR]: Alvo capturado\"\n",
    "            # remover da lista esta na classe de treinamento\n",
    "            self.current_target = None\n",
    "\n",
    "        elif target_detected and 0 < closest_target_distance <= 3:\n",
    "            # Recompensa por proximidade com a presa\n",
    "            if closest_target_distance == 3:\n",
    "                reward += 0.5\n",
    "            elif closest_target_distance == 2:\n",
    "                reward += 1.0\n",
    "            elif closest_target_distance == 1:\n",
    "                reward += 2.0\n",
    "            feedback = f\"[PREDATOR]: Proximidade presa: {closest_target_distance}\"\n",
    "\n",
    "        elif ally_detected and 0 < closest_ally_distance <= 3:\n",
    "            # Recompensa por proximidade com aliado (que tem informação de presa)\n",
    "            if closest_ally_distance == 3:\n",
    "                reward += 0.1\n",
    "            elif closest_ally_distance == 2:\n",
    "                reward += 0.3\n",
    "            elif closest_ally_distance == 1:\n",
    "                reward += 0.5\n",
    "            feedback = f\"[PREDATOR]: Proximidade aliado: {closest_ally_distance}\"\n",
    "\n",
    "        else:\n",
    "            feedback += \"[PREDATOR]: Explorando mapa\"\n",
    "\n",
    "        # Atualiza channel\n",
    "        for agent in self.env.agents:\n",
    "            if agent.is_alive and agent.breed == 2:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray:\n",
    "                    self.channel = [1.0, 1.0, 0.0]\n",
    "                    break\n",
    "                else:\n",
    "                    self.channel = [1.0, 0.0, 0.0]\n",
    "\n",
    "        return done, reward, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: Obstacle\n",
    "# ==========================================\n",
    "\n",
    "class Obstacle:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.channel = [0.0, 1.0, 0.0]  # Cor verde para o obstáculo (RGB)\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.channel = [0.0, 1.0, 0.0]\n",
    "\n",
    "        # Escolhe uma posição aleatória no grid que esteja livre\n",
    "        position = env.new_position()\n",
    "        if position:\n",
    "            self.x, self.y = position\n",
    "        else:\n",
    "            raise ValueError(\"Não foi possível encontrar uma posição livre para o obstáculo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: Environment\n",
    "# ==========================================\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, sizeX, sizeY, ray=3):\n",
    "        self.sizeX = sizeX\n",
    "        self.sizeY = sizeY\n",
    "        self.ray = ray\n",
    "        self.agents = []\n",
    "        self.objects = []\n",
    "        self.obstacles = []  # Lista separada para obstáculos\n",
    "        self.actions = 9  # Número de ações possíveis\n",
    "\n",
    "\n",
    "    def new_position(self):\n",
    "        # Cria uma lista de todas as posições possíveis.\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = list(itertools.product(*iterables))\n",
    "\n",
    "        # Cria uma lista das posições atuais ocupadas pelos agentes e obstáculos.\n",
    "        current_positions = [(agent.x, agent.y) for agent in self.agents]\n",
    "        current_positions += [(obstacle.x, obstacle.y) for obstacle in self.obstacles]\n",
    "\n",
    "        # Filtra as posições possíveis, removendo as ocupadas.\n",
    "        available_points = [point for point in points if point not in current_positions]\n",
    "\n",
    "        # Escolhe aleatoriamente uma das posições disponíveis.\n",
    "        if available_points:\n",
    "            return random.choice(available_points)\n",
    "        else:\n",
    "            # Loga a mensagem indicando que não há posições disponíveis.\n",
    "            logging.info(\"Não foi possível encontrar uma nova posição disponível.\")\n",
    "            return None\n",
    "\n",
    "    def add_obstacle(self, obstacle):\n",
    "        # Define uma posição disponível para o obstáculo\n",
    "        position = self.new_position()\n",
    "        if position:\n",
    "            obstacle.x, obstacle.y = position  # Atribui as coordenadas ao obstáculo\n",
    "            self.objects.append(obstacle)\n",
    "        else:\n",
    "            logging.info(\"Não foi possível adicionar um novo obstáculo: nenhuma posição disponível.\")\n",
    "\n",
    "    def add_agent(self, agent):\n",
    "        # Define uma posição disponível para o agente\n",
    "        position = self.new_position()\n",
    "        if position:\n",
    "            agent.x, agent.y = position  # Atribui as coordenadas ao agente\n",
    "            self.objects.append(agent)\n",
    "        else:\n",
    "            logging.info(\"Não foi possível adicionar um novo agente: nenhuma posição disponível.\")\n",
    "\n",
    "    def reset(self):\n",
    "        # Verifica se a lista de objetos está vazia\n",
    "        if not self.objects:\n",
    "            logging.info(\"Não é possível prosseguir: nenhum objeto foi adicionado ao ambiente.\")\n",
    "            return  # Interrompe o método se `self.objects` estiver vazio\n",
    "\n",
    "        # Limpa as listas de agentes e obstáculos para reiniciar o ambiente\n",
    "        self.agents = []\n",
    "        self.obstacles = []\n",
    "\n",
    "        # Embaralha a lista de objetos para variar a ordem dos elementos no ambiente\n",
    "        random.shuffle(self.objects)\n",
    "\n",
    "        # Adiciona os agentes e obstáculos à lista apropriada e os reseta\n",
    "        for item in self.objects:\n",
    "            item.reset(self)  # Reposiciona cada objeto no ambiente\n",
    "            if isinstance(item, Agent):  # Considerando uma classe Agent\n",
    "                self.agents.append(item)\n",
    "            elif isinstance(item, Obstacle):  # Considerando uma classe Obstacle\n",
    "                self.obstacles.append(item)\n",
    "\n",
    "    def is_position_empty_and_valid(self, x, y):\n",
    "        # Verifica se a posição está dentro dos limites do ambiente\n",
    "        if x < 0 or x >= self.sizeX or y < 0 or y >= self.sizeY:\n",
    "            return False  # A posição está fora dos limites do ambiente\n",
    "\n",
    "        # Verifica se a posição está ocupada por algum agente\n",
    "        for agent in self.agents:\n",
    "            if agent.x == x and agent.y == y:\n",
    "                return False  # A posição está ocupada\n",
    "\n",
    "        # Verifica se a posição está ocupada por algum obstáculo\n",
    "        for obstacle in self.obstacles:\n",
    "            if obstacle.x == x and obstacle.y == y:\n",
    "                return False  # A posição está ocupada por um obstáculo\n",
    "\n",
    "    def get_agent_at_position(self, x, y):\n",
    "        for agent in self.agents:\n",
    "            if agent.x == x and agent.y == y:\n",
    "                return agent\n",
    "        return None\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Inicializa a penalidade padrão e os valores de penalidade\n",
    "        ZERO = 0.0\n",
    "        PENALIZE = -10.0\n",
    "        direction = action\n",
    "\n",
    "        # Inicializa os incrementos de movimento\n",
    "        new_x, new_y = 0, 0\n",
    "\n",
    "        # Define os incrementos de movimento com base na direção\n",
    "        if direction == 0:  # Para cima\n",
    "            new_y = -1\n",
    "        elif direction == 1:  # Para cima e direita (diagonal)\n",
    "            new_x = 1\n",
    "            new_y = -1\n",
    "        elif direction == 2:  # Para direita\n",
    "            new_x = 1\n",
    "        elif direction == 3:  # Para baixo e direita (diagonal)\n",
    "            new_x = 1\n",
    "            new_y = 1\n",
    "        elif direction == 4:  # Para baixo\n",
    "            new_y = 1\n",
    "        elif direction == 5:  # Para baixo e esquerda (diagonal)\n",
    "            new_x = -1\n",
    "            new_y = 1\n",
    "        elif direction == 6:  # Para esquerda\n",
    "            new_x = -1\n",
    "        elif direction == 7:  # Para cima e esquerda (diagonal)\n",
    "            new_x = -1\n",
    "            new_y = -1\n",
    "        elif direction == 8:  # Ficar parado\n",
    "            new_x = 0\n",
    "            new_y = 0\n",
    "\n",
    "        # Calcula a nova posição absoluta do agente\n",
    "        target_x = agent.x + new_x\n",
    "        target_y = agent.y + new_y\n",
    "\n",
    "        # Verifica se a nova posição contém um obstáculo\n",
    "        if any(obstacle.x == target_x and obstacle.y == target_y for obstacle in self.obstacles):\n",
    "            print(\"Agente tentou ocupar um obstáculo!\")\n",
    "            return PENALIZE  # Penalidade por tentar ocupar a posição de um obstáculo\n",
    "\n",
    "        # Verifica se o movimento está dentro dos limites do ambiente\n",
    "        if target_x < 0 or target_x >= self.sizeX or target_y < 0 or target_y >= self.sizeY:\n",
    "            print(\"Agente fora do limite!\")\n",
    "            return PENALIZE\n",
    "\n",
    "        # Verifica se a nova posição contém outro agente\n",
    "        other_agent = self.get_agent_at_position(target_x, target_y)\n",
    "        if other_agent:\n",
    "            # Lógica para a presa\n",
    "            if agent.breed == 2:  # Presa\n",
    "                print(\"Presa não pode ocupar a posição de outro agente!\")\n",
    "                return PENALIZE  # Penalidade por tentar ocupar a posição de outro agente\n",
    "\n",
    "            # Lógica para o predador\n",
    "            elif agent.breed == 0:  # Predador\n",
    "                if other_agent.breed == 0:  # Outro predador na posição\n",
    "                    print(\"Predador não pode ocupar a posição de outro predador!\")\n",
    "                    return PENALIZE  # Penalidade por tentar ocupar a posição de outro predador\n",
    "\n",
    "        # Atualiza a posição do agente se o movimento for válido\n",
    "        agent.x, agent.y = target_x, target_y\n",
    "        return ZERO\n",
    "\n",
    "    def render_env(self):\n",
    "        a = np.zeros([self.sizeY, self.sizeX, 3])\n",
    "\n",
    "        for agent in self.agents:\n",
    "            if agent.x is not None and agent.y is not None:\n",
    "                a[agent.y, agent.x, :] = agent.channel\n",
    "\n",
    "        for obstacle in self.obstacles:\n",
    "            a[obstacle.y, obstacle.x, :] = obstacle.channel  # Renderiza obstáculos\n",
    "\n",
    "        return a\n",
    "\n",
    "    def render_agent(self, agent):\n",
    "        # Renderiza o ambiente para obter a matriz RGB atual\n",
    "        a = self.render_env()\n",
    "\n",
    "        # Calcula o tamanho do recorte com base em self.ray\n",
    "        recorte_tamanho = 2 * self.ray + 1\n",
    "\n",
    "        # Inicializa o recorte temporário com cor amarela\n",
    "        recorte_temp = np.ones((recorte_tamanho, recorte_tamanho, 3)) * np.array([1.0, 1.0, 1.0])  # fora do Grid - white\n",
    "\n",
    "        # Calcula as coordenadas do recorte dentro do ambiente\n",
    "        inicio_x = agent.x - self.ray\n",
    "        inicio_y = agent.y - self.ray\n",
    "        fim_x = inicio_x + recorte_tamanho\n",
    "        fim_y = inicio_y + recorte_tamanho\n",
    "\n",
    "        # Calcula os limites de sobreposição entre o recorte e o ambiente\n",
    "        sobreposicao_inicio_x = max(inicio_x, 0)\n",
    "        sobreposicao_inicio_y = max(inicio_y, 0)\n",
    "        sobreposicao_fim_x = min(fim_x, self.sizeX)\n",
    "        sobreposicao_fim_y = min(fim_y, self.sizeY)\n",
    "\n",
    "        # Calcula os índices de destino no recorte temporário\n",
    "        destino_inicio_x = sobreposicao_inicio_x - inicio_x\n",
    "        destino_inicio_y = sobreposicao_inicio_y - inicio_y\n",
    "        destino_fim_x = destino_inicio_x + sobreposicao_fim_x - sobreposicao_inicio_x\n",
    "        destino_fim_y = destino_inicio_y + sobreposicao_fim_y - sobreposicao_inicio_y\n",
    "\n",
    "        # Copia a sobreposição do ambiente para o recorte temporário\n",
    "        recorte_temp[destino_inicio_y:destino_fim_y, destino_inicio_x:destino_fim_x] = \\\n",
    "            a[sobreposicao_inicio_y:sobreposicao_fim_y, sobreposicao_inicio_x:sobreposicao_fim_x]\n",
    "\n",
    "        # Pinta o elemento central do recorte de branco, ajustando a posição baseada em self.ray\n",
    "        centro = self.ray\n",
    "        recorte_temp[centro, centro, :] = np.array([0.5, 0.5, 0.5])  # Cinza - Destaque de Agente em treinamento\n",
    "\n",
    "        return recorte_temp\n",
    "\n",
    "    def population_count(self):\n",
    "        \"\"\"Retorna a quantidade de presas, predadores e obstáculos no ambiente.\"\"\"\n",
    "        predator_count = 0\n",
    "        prey_count = 0\n",
    "\n",
    "        for agent in self.agents:\n",
    "            if agent.is_alive is True:\n",
    "                if agent.breed == 0:\n",
    "                    predator_count += 1\n",
    "                elif agent.breed == 2:\n",
    "                    prey_count += 1\n",
    "\n",
    "        obstacle_count = len(self.obstacles)  # Conta o total de obstáculos na lista `self.obstacles`\n",
    "        return prey_count, predator_count, obstacle_count\n",
    "\n",
    "    def remove_agent(self, agent):\n",
    "        self.agents.remove(agent)\n",
    "\n",
    "    @staticmethod\n",
    "    def chebyshev_distance(x1, y1, x2, y2):\n",
    "        distance = max(abs(x2 - x1), abs(y2 - y1))\n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support\n",
    "# ==========================================\n",
    "\n",
    "lr = 0.0001\n",
    "l2_regularization = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class - RADAR\n",
    "# ==========================================\n",
    "\n",
    "class ColorCombDepthwiseConv2D(Layer):\n",
    "    def __init__(self, kernel_size=(7, 7), activation='relu', padding='same', **kwargs):\n",
    "        super(ColorCombDepthwiseConv2D, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.padding = padding\n",
    "\n",
    "        # Convoluções individuais para cores puras\n",
    "        self.conv_r = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_r\")\n",
    "        self.bn_r = BatchNormalization(name=\"bn_r\")\n",
    "        \n",
    "        self.conv_g = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_g\")\n",
    "        self.bn_g = BatchNormalization(name=\"bn_g\")\n",
    "        \n",
    "        self.conv_b = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_b\")\n",
    "        self.bn_b = BatchNormalization(name=\"bn_b\")\n",
    "\n",
    "        # Convoluções para combinações específicas (Magenta, Ciano, Amarelo)\n",
    "        self.conv_magenta = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_magenta\")\n",
    "        self.bn_magenta = BatchNormalization(name=\"bn_magenta\")\n",
    "        \n",
    "        self.conv_cyan = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_cyan\")\n",
    "        self.bn_cyan = BatchNormalization(name=\"bn_cyan\")\n",
    "        \n",
    "        self.conv_yellow = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_yellow\")\n",
    "        self.bn_yellow = BatchNormalization(name=\"bn_yellow\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        r, g, b = tf.split(inputs, num_or_size_splits=3, axis=-1)\n",
    "\n",
    "        # Convoluções individuais com BatchNormalization\n",
    "        r_out = self.bn_r(self.conv_r(r), training=training)\n",
    "        g_out = self.bn_g(self.conv_g(g), training=training)\n",
    "        b_out = self.bn_b(self.conv_b(b), training=training)\n",
    "\n",
    "        # Combinações de canais com BatchNormalization\n",
    "        magenta_out = self.bn_magenta(self.conv_magenta(r + b), training=training)\n",
    "        cyan_out = self.bn_cyan(self.conv_cyan(g + b), training=training)\n",
    "        yellow_out = self.bn_yellow(self.conv_yellow(r + g), training=training)\n",
    "\n",
    "        # Concatenando saídas\n",
    "        outputs = Concatenate(axis=-1, name=\"concat_colors\")([r_out, g_out, b_out, magenta_out, cyan_out, yellow_out])\n",
    "\n",
    "        # Ativação\n",
    "        outputs = Activation(self.activation, name=\"activation_colors\")(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SpatialAttentionModule(Layer):\n",
    "    def __init__(self, kernel_size=4):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv = Conv2D(1, kernel_size=kernel_size, padding='same', activation=None, name=\"attention_conv\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        squared_inputs = K.square(inputs)\n",
    "        l2_pool = tf.sqrt(tf.reduce_mean(squared_inputs, axis=-1, keepdims=True) + 1e-6)\n",
    "\n",
    "        concat = Concatenate(axis=-1, name=\"concat_attention\")([avg_pool, l2_pool])\n",
    "        attention_map = self.conv(concat)\n",
    "        attention_map = Activation('sigmoid', name=\"sigmoid_attention\")(attention_map)\n",
    "        return Multiply(name=\"apply_attention\")([inputs, attention_map])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_dqn\n",
    "# ==========================================\n",
    "\n",
    "class nn_dqn(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_dqn, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camadas convolucionais\n",
    "        self.input_layer = InputLayer(input_shape=input_shape)\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout após conv1\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout após conv2\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.5, name=\"dropout_dense1_layer\")  # Dropout após dense1\n",
    "        \n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.5, name=\"dropout_dense2_layer\")  # Dropout após dense2\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado após conv1\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado após conv2\n",
    "\n",
    "        # Atenção espacial e Flatten\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas densas\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Dropout aplicado após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Dropout aplicado após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_per\n",
    "# ==========================================\n",
    "\n",
    "class nn_per(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_per, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.3, name=\"dropout_flatten_layer\")\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.4, name=\"dropout_dense1_layer\")\n",
    "\n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.4, name=\"dropout_dense2_layer\")\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Aplicação do Dropout após conv1\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Aplicação do Dropout após conv2\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Aplicação do Dropout após Flatten\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Aplicação do Dropout após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Aplicação do Dropout após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ, weights = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            td_errors = targetQ - Q\n",
    "            weighted_loss = tf.reduce_mean(weights * tf.square(td_errors))\n",
    "\n",
    "        grads = tape.gradient(weighted_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return weighted_loss  # Apenas a perda\n",
    "\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_dueling\n",
    "# ==========================================\n",
    "\n",
    "class nn_dueling(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_dueling, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "        \n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout na primeira camada convolucional\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout na segunda camada convolucional\n",
    "\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        # Camadas densas compartilhadas\n",
    "        self.dense_shared1 = Dense(64, activation='relu', name=\"shared_dense1\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_shared = Dropout(0.4, name=\"dropout_shared_layer\")  # Dropout na camada compartilhada\n",
    "\n",
    "        # Rede para Valor (V)\n",
    "        self.value_dense = Dense(32, activation='relu', name=\"value_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_value = Dropout(0.4, name=\"dropout_value_layer\")  # Dropout na rede de valor\n",
    "        self.value_output = Dense(1, activation='linear', name=\"value_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Rede para Vantagem (A)\n",
    "        self.advantage_dense = Dense(32, activation='relu', name=\"advantage_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_advantage = Dropout(0.4, name=\"dropout_advantage_layer\")  # Dropout na rede de vantagem\n",
    "        self.advantage_output = Dense(num_actions, activation='linear', name=\"advantage_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado\n",
    "        \n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas compartilhadas\n",
    "        x = self.dense_shared1(x)\n",
    "        x = self.dropout_shared(x, training=training)  # Dropout aplicado na camada compartilhada\n",
    "\n",
    "        # Valor (V)\n",
    "        v = self.value_dense(x)\n",
    "        v = self.dropout_value(v, training=training)  # Dropout aplicado na rede de valor\n",
    "        v = self.value_output(v)\n",
    "\n",
    "        # Vantagem (A)\n",
    "        a = self.advantage_dense(x)\n",
    "        a = self.dropout_advantage(a, training=training)  # Dropout aplicado na rede de vantagem\n",
    "        a = self.advantage_output(a)\n",
    "\n",
    "        # Combina V e A para calcular Q\n",
    "        q = v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return q\n",
    "    \n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_double\n",
    "# ==========================================\n",
    "\n",
    "class nn_double(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_double, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Otimizador e função de perda\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Camada de entrada e processamento inicial de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "\n",
    "        # Camadas convolucionais da rede principal\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "\n",
    "        # Camadas densas\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Camadas da rede-alvo (adicionadas as mesmas modificações)\n",
    "        self.target_conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm1 = BatchNormalization()\n",
    "        self.target_relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm2 = BatchNormalization()\n",
    "        self.target_relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "        self.target_flatten = Flatten()\n",
    "        self.target_dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede principal para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def target_call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede-alvo para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        \n",
    "\n",
    "        x = self.target_conv1(x)\n",
    "        x = self.target_batch_norm1(x, training=training)\n",
    "        x = self.target_relu1(x)\n",
    "\n",
    "        x = self.target_conv2(x)\n",
    "        x = self.target_batch_norm2(x, training=training)\n",
    "        x = self.target_relu2(x)\n",
    "\n",
    "        \n",
    "        x = self.target_flatten(x)\n",
    "        x = self.target_dense1(x)\n",
    "\n",
    "        x = self.target_dense2(x)\n",
    "        return self.target_output_layer(x)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Sincroniza os pesos da rede principal para a rede-alvo.\"\"\"\n",
    "        self.target_conv1.set_weights(self.conv1.get_weights())\n",
    "        self.target_batch_norm1.set_weights(self.batch_norm1.get_weights())\n",
    "        self.target_conv2.set_weights(self.conv2.get_weights())\n",
    "        self.target_batch_norm2.set_weights(self.batch_norm2.get_weights())\n",
    "        self.target_dense1.set_weights(self.dense1.get_weights())\n",
    "        self.target_dense2.set_weights(self.dense2.get_weights())\n",
    "        self.target_output_layer.set_weights(self.output_layer.get_weights())\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        \"\"\"Realiza uma etapa de treinamento com Double DQN.\"\"\"\n",
    "        states, actions, targetQ = batch_data\n",
    "        next_states, rewards, dones = targetQ\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predições da rede principal\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "\n",
    "            # Double DQN: Calcula o valor-alvo\n",
    "            main_Q_values_next = self(next_states)  # Rede principal para selecionar a melhor ação\n",
    "            next_actions = tf.argmax(main_Q_values_next, axis=1)\n",
    "            target_Q_values_next = self.target_call(next_states)  # Rede-alvo para calcular Q\n",
    "            target_Q = tf.reduce_sum(target_Q_values_next * tf.one_hot(next_actions, self.num_actions), axis=1)\n",
    "            target_Q = rewards + (1 - dones) * self.discount_factor * target_Q\n",
    "\n",
    "            # Calcula o loss\n",
    "            loss = self.loss_fn(target_Q, Q)\n",
    "\n",
    "        # Gradientes e atualização dos pesos\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Prediz a melhor ação para um dado estado.\"\"\"\n",
    "        Q_values = self(tf.expand_dims(state, axis=0))\n",
    "        return tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"Salva os pesos do modelo principal.\"\"\"\n",
    "        self.save_weights(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"Carrega os pesos do modelo principal.\"\"\"\n",
    "        self.load_weights(file_path)\n",
    "        print(f\"Modelo carregado de: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_dqn\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_dqn(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_radar_dqn, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camadas convolucionais\n",
    "        self.input_layer = InputLayer(input_shape=input_shape)\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout após conv1\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout após conv2\n",
    "\n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.5, name=\"dropout_dense1_layer\")  # Dropout após dense1\n",
    "        \n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.5, name=\"dropout_dense2_layer\")  # Dropout após dense2\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado após conv1\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado após conv2\n",
    "\n",
    "        # Atenção espacial e Flatten\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas densas\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Dropout aplicado após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Dropout aplicado após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_per\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_per(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_radar_per, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")\n",
    "        \n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.3, name=\"dropout_flatten_layer\")\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.4, name=\"dropout_dense1_layer\")\n",
    "\n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.4, name=\"dropout_dense2_layer\")\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Aplicação do Dropout após conv1\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Aplicação do Dropout após conv2\n",
    "\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Aplicação do Dropout após Flatten\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Aplicação do Dropout após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Aplicação do Dropout após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ, weights = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            td_errors = targetQ - Q\n",
    "            weighted_loss = tf.reduce_mean(weights * tf.square(td_errors))\n",
    "\n",
    "        grads = tape.gradient(weighted_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return weighted_loss  # Apenas a perda\n",
    "\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_dueling\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_dueling(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_radar_dueling, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "        \n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout na primeira camada convolucional\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout na segunda camada convolucional\n",
    "     \n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        # Camadas densas compartilhadas\n",
    "        self.dense_shared1 = Dense(64, activation='relu', name=\"shared_dense1\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_shared = Dropout(0.4, name=\"dropout_shared_layer\")  # Dropout na camada compartilhada\n",
    "\n",
    "        # Rede para Valor (V)\n",
    "        self.value_dense = Dense(32, activation='relu', name=\"value_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_value = Dropout(0.4, name=\"dropout_value_layer\")  # Dropout na rede de valor\n",
    "        self.value_output = Dense(1, activation='linear', name=\"value_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Rede para Vantagem (A)\n",
    "        self.advantage_dense = Dense(32, activation='relu', name=\"advantage_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_advantage = Dropout(0.4, name=\"dropout_advantage_layer\")  # Dropout na rede de vantagem\n",
    "        self.advantage_output = Dense(num_actions, activation='linear', name=\"advantage_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado\n",
    "        \n",
    "        # Atenção espacial\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas compartilhadas\n",
    "        x = self.dense_shared1(x)\n",
    "        x = self.dropout_shared(x, training=training)  # Dropout aplicado na camada compartilhada\n",
    "\n",
    "        # Valor (V)\n",
    "        v = self.value_dense(x)\n",
    "        v = self.dropout_value(v, training=training)  # Dropout aplicado na rede de valor\n",
    "        v = self.value_output(v)\n",
    "\n",
    "        # Vantagem (A)\n",
    "        a = self.advantage_dense(x)\n",
    "        a = self.dropout_advantage(a, training=training)  # Dropout aplicado na rede de vantagem\n",
    "        a = self.advantage_output(a)\n",
    "\n",
    "        # Combina V e A para calcular Q\n",
    "        q = v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return q\n",
    "    \n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_double\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_double(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_radar_double, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Otimizador e função de perda\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Camada de entrada e processamento inicial de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais da rede principal\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        # Módulo de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Camadas densas\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Camadas da rede-alvo (adicionadas as mesmas modificações)\n",
    "        self.target_color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"target_color_comb_layer\")\n",
    "        self.target_conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm1 = BatchNormalization()\n",
    "        self.target_relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm2 = BatchNormalization()\n",
    "        self.target_relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        self.target_flatten = Flatten()\n",
    "        self.target_dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede principal para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def target_call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede-alvo para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.target_color_comb_layer(x)\n",
    "\n",
    "        x = self.target_conv1(x)\n",
    "        x = self.target_batch_norm1(x, training=training)\n",
    "        x = self.target_relu1(x)\n",
    "\n",
    "        x = self.target_conv2(x)\n",
    "        x = self.target_batch_norm2(x, training=training)\n",
    "        x = self.target_relu2(x)\n",
    "\n",
    "        x = self.target_spatial_attention(x)\n",
    "        x = self.target_flatten(x)\n",
    "        x = self.target_dense1(x)\n",
    "\n",
    "        x = self.target_dense2(x)\n",
    "        return self.target_output_layer(x)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Sincroniza os pesos da rede principal para a rede-alvo.\"\"\"\n",
    "        self.target_color_comb_layer.set_weights(self.color_comb_layer.get_weights())\n",
    "        self.target_conv1.set_weights(self.conv1.get_weights())\n",
    "        self.target_batch_norm1.set_weights(self.batch_norm1.get_weights())\n",
    "        self.target_conv2.set_weights(self.conv2.get_weights())\n",
    "        self.target_batch_norm2.set_weights(self.batch_norm2.get_weights())\n",
    "        self.target_dense1.set_weights(self.dense1.get_weights())\n",
    "        self.target_dense2.set_weights(self.dense2.get_weights())\n",
    "        self.target_output_layer.set_weights(self.output_layer.get_weights())\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        \"\"\"Realiza uma etapa de treinamento com Double DQN.\"\"\"\n",
    "        states, actions, targetQ = batch_data\n",
    "        next_states, rewards, dones = targetQ\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predições da rede principal\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "\n",
    "            # Double DQN: Calcula o valor-alvo\n",
    "            main_Q_values_next = self(next_states)  # Rede principal para selecionar a melhor ação\n",
    "            next_actions = tf.argmax(main_Q_values_next, axis=1)\n",
    "            target_Q_values_next = self.target_call(next_states)  # Rede-alvo para calcular Q\n",
    "            target_Q = tf.reduce_sum(target_Q_values_next * tf.one_hot(next_actions, self.num_actions), axis=1)\n",
    "            target_Q = rewards + (1 - dones) * self.discount_factor * target_Q\n",
    "\n",
    "            # Calcula o loss\n",
    "            loss = self.loss_fn(target_Q, Q)\n",
    "\n",
    "        # Gradientes e atualização dos pesos\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Prediz a melhor ação para um dado estado.\"\"\"\n",
    "        Q_values = self(tf.expand_dims(state, axis=0))\n",
    "        return tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"Salva os pesos do modelo principal.\"\"\"\n",
    "        self.save_weights(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"Carrega os pesos do modelo principal.\"\"\"\n",
    "        self.load_weights(file_path)\n",
    "        print(f\"Modelo carregado de: {file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMULATION\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support\n",
    "# ==========================================\n",
    "\n",
    "def save_sim(task_name, episode, step, tot_predator, tot_prey, reward_total, done_total, output_dir=\"output_logs\"):\n",
    "    \"\"\"\n",
    "    Salva informações de treinamento em arquivos separados para cada tipo de agente (prey ou predator).\n",
    "    \"\"\"\n",
    "    # Verifica e cria o diretório, se necessário\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Itera pelos tipos de agentes ('predator' e 'prey')\n",
    "    for agent_type in ['predator', 'prey']:\n",
    "        # Define o nome do arquivo com base no tipo de agente\n",
    "        file_name = os.path.join(output_dir, f\"{task_name}_{agent_type}_simlog.txt\")\n",
    "\n",
    "        # Constrói a linha de texto a ser gravada\n",
    "        line = (\n",
    "            f\"Episode: {episode}, Step: {step}, \"\n",
    "            f\"Population (Prey/Predator): {tot_prey}/{tot_predator}, \"\n",
    "            f\"Total Reward ({agent_type}): {reward_total[agent_type]:.2f}, \"\n",
    "            f\"Total Dones ({agent_type}): {done_total[agent_type]}\\n\"\n",
    "        )\n",
    "\n",
    "        # Abre o arquivo no modo de adição e escreve a linha\n",
    "        with open(file_name, \"a\") as file:\n",
    "            file.write(line)\n",
    "\n",
    "def normalize_channel(channel):\n",
    "\n",
    "    if isinstance(channel, (list, tuple)):\n",
    "        return tuple(int(value * 255) for value in channel)\n",
    "    raise TypeError(\"O canal deve ser uma lista ou tupla de valores.\")\n",
    "\n",
    "def footprints(obstacles, agents, task_name, episode, step, grid_width, grid_height, save_obstacles=False):\n",
    "    \"\"\"\n",
    "    Atualizado para incluir grid_width e grid_height em cada linha.\n",
    "    Remove o arquivo existente se necessário.\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(\".\", \"final-models\", \"monitor\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    file_name = os.path.join(model_dir, f\"sim_{task_name}_footprints.csv\")\n",
    "    header = \"Episode,Step,Type,Is_Alive,R,G,B,X,Y,Grid_Width,Grid_Height\\n\"\n",
    "\n",
    "    # Remove o arquivo existente se necessário\n",
    "    if os.path.exists(file_name) and step == 0 and episode == 0:\n",
    "        os.remove(file_name)\n",
    "\n",
    "    # Carrega os dados existentes e inicializa os índices dos agentes\n",
    "    agent_indices = {}\n",
    "    existing_data = set()\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            if len(lines) > 1:\n",
    "                for line in lines[1:]:\n",
    "                    existing_data.add(line.strip())\n",
    "                    index, _, _, agent_name, *_ = line.split(\",\")\n",
    "                    agent_indices[agent_name] = int(index)\n",
    "\n",
    "    # Define o modo de abertura do arquivo\n",
    "    file_mode = \"w\" if not os.path.exists(file_name) else \"a\"\n",
    "    index = max(agent_indices.values(), default=0) + 1\n",
    "\n",
    "    with open(file_name, file_mode) as file:\n",
    "        if file_mode == \"w\":\n",
    "            file.write(header)\n",
    "\n",
    "        # Grava obstáculos\n",
    "        if save_obstacles and step == 0:\n",
    "            for obstacle in obstacles:\n",
    "                normalized_channel = normalize_channel(obstacle.channel)\n",
    "                line = (\n",
    "                    f\"{episode},{step},Obstacle,True,\"\n",
    "                    f\"{normalized_channel[0]},{normalized_channel[1]},{normalized_channel[2]},\"\n",
    "                    f\"{obstacle.x},{obstacle.y},{grid_width},{grid_height}\\n\"\n",
    "                )\n",
    "                if line.strip() not in existing_data:\n",
    "                    file.write(line)\n",
    "                    index += 1\n",
    "\n",
    "        # Grava agentes\n",
    "        for agent in agents:\n",
    "            normalized_channel = normalize_channel(agent.channel)\n",
    "            is_alive = \"True\" if agent.is_alive else \"False\"\n",
    "            line = (\n",
    "                f\"{episode},{step},{agent.name},{is_alive},\"\n",
    "                f\"{normalized_channel[0]},{normalized_channel[1]},{normalized_channel[2]},\"\n",
    "                f\"{agent.x},{agent.y},{grid_width},{grid_height}\\n\"\n",
    "            )\n",
    "            if line.strip() not in existing_data:\n",
    "                file.write(line)\n",
    "                agent_indices[agent.name] = index\n",
    "                index += 1\n",
    "\n",
    "def save_log_to_file(filename, episode, step, agent, pop_prey, pop_predator, action, reward, done, nearby, feedback):\n",
    "    # Formate a string como no print original\n",
    "    log_entry = (f\"{episode}/{step}: {agent.name}, Pop: {pop_prey}/{pop_predator}, \"\n",
    "                 f\"Breed:{agent.breed}, Channel: {agent.channel} Position: ({agent.x}, {agent.y}), Action: {action}, \"\n",
    "                 f\"Reward: {reward}, Done: {done}, Nearby: {nearby}, Feedback: {feedback}\")\n",
    "    \n",
    "    # Abra o arquivo no modo append para não sobrescrever logs antigos\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(log_entry + '\\n')  # Adicione uma nova linha após cada entrada\n",
    "\n",
    "def get_nearby_agents_count(agent, agents_list, max_distance=3):\n",
    "    \"\"\"\n",
    "    Conta quantos agentes estão próximos a um agente específico, dentro de um raio especificado.\n",
    "\n",
    "    Args:\n",
    "        agent (object): O agente para o qual calcular a proximidade.\n",
    "        agents_list (list): Lista de agentes no ambiente.\n",
    "        max_distance (int): Distância máxima para considerar como \"próximo\". Default é 3.\n",
    "\n",
    "    Returns:\n",
    "        int: Número de agentes dentro da distância especificada do agente fornecido.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for other_agent in agents_list:\n",
    "        # Ignorar o próprio agente e agentes que não estão vivos\n",
    "        if other_agent == agent or agent.breed != other_agent.breed or not other_agent.is_alive:\n",
    "            continue\n",
    "        \n",
    "        # Calcular a distância de Chebyshev\n",
    "        distance = max(abs(agent.x - other_agent.x), abs(agent.y - other_agent.y))\n",
    "        \n",
    "        if distance <= max_distance:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIM\n",
    "# ==========================================\n",
    "\n",
    "class Sim:\n",
    "    def __init__(self, task_name, category, env, num_predators, num_preys, limit_pop_predator, limit_pop_prey, model_nn_predator, model_nn_prey, num_episodes=100, num_steps=10):\n",
    "        self.directory = f\"C:/Users/beLIVE/IA/DECISYS\"\n",
    "        self.model_dir = os.path.join(\".\", \"final-models\", \"sim\")\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        self.category = category\n",
    "        self.task_name = task_name\n",
    "        self.env = env\n",
    "        self.num_predators = num_predators\n",
    "        self.num_preys = num_preys\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.model_nn_predator = model_nn_predator\n",
    "        self.model_nn_prey = model_nn_prey\n",
    "        self.count_episodes = 0\n",
    "        self.limit_pop_predator = limit_pop_predator\n",
    "        self.limit_pop_prey = limit_pop_prey\n",
    "\n",
    "    def run(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            self.env.reset()\n",
    "\n",
    "            episode_rewards = {'predator': 0, 'prey': 0}\n",
    "            episode_dones = {'predator': 0, 'prey': 0}\n",
    "\n",
    "            # Define a flag para salvar os obstáculos no início do episódio\n",
    "            save_obstacles_flag = True\n",
    "\n",
    "            for step in range(self.num_steps):\n",
    "                if not any(a.breed == 2 and a.is_alive for a in self.env.agents) or not any(a.breed == 0 and a.is_alive for a in self.env.agents):\n",
    "                    break\n",
    "\n",
    "                for agent in self.env.agents:\n",
    "                    if not agent.is_alive:\n",
    "                        continue\n",
    "\n",
    "                    # Render state and predict action\n",
    "                    state = self.env.render_agent(agent)\n",
    "                    state_expanded = np.expand_dims(state, 0)\n",
    "                    model = self.model_nn_prey if agent.breed == 2 else self.model_nn_predator\n",
    "                    Q_values = model.predict(state_expanded, verbose=0)\n",
    "\n",
    "                    \n",
    "                    if self.category == \"prqipydb\":\n",
    "                       action = np.argmax(Q_values[0]) if agent.breed == 0 else np.random.randint(0, 9)\n",
    "                    elif self.category == \"prdbpyqi\":\n",
    "                        action = np.argmax(Q_values[0]) if agent.breed == 2 else np.random.randint(0, 9)\n",
    "                    else: \n",
    "                        action = np.argmax(Q_values[0])\n",
    "                        \n",
    "\n",
    "                    # Perform action\n",
    "                    _, reward, done, feedback = agent.step(action)\n",
    "\n",
    "\n",
    "                    # Atualiza os dados de recompensas e terminais\n",
    "                    if agent.breed == 0:  # Predador\n",
    "                        episode_rewards['predator'] += reward\n",
    "                        episode_dones['predator'] += 1 if done else 0\n",
    "                    elif agent.breed == 2:  # Presa\n",
    "                        episode_rewards['prey'] += reward\n",
    "                        episode_dones['prey'] += 1 if done else 0\n",
    "\n",
    "                    agents_list = self.env.agents\n",
    "                    nearby = get_nearby_agents_count(agent, agents_list)\n",
    "                    pop_prey, pop_predator, _ = self.env.population_count()\n",
    "                    # Print agent details\n",
    "                    print(f\"{episode}/{step}: {agent.name}, Pop: {pop_prey}/{pop_predator}, \"\n",
    "                          f\"Breed:{agent.breed}, Color: {agent.channel}  Position: ({agent.x}, {agent.y}), Action: {action}, Reward: {reward}, Done: {done}, Nearby: {nearby}, Feedback: {feedback}\")\n",
    "                    save_log_to_file(self.task_name, episode, step, agent, pop_prey, pop_predator, action, reward, done, nearby, feedback)\n",
    "\n",
    "                # Salva os footprints dos agentes a cada passo\n",
    "                footprints(self.env.obstacles, self.env.agents, self.task_name, episode, step, self.env.sizeX, self.env.sizeY,save_obstacles=save_obstacles_flag)\n",
    "\n",
    "                # Após salvar os obstáculos pela primeira vez, desativa a flag\n",
    "                if save_obstacles_flag: save_obstacles_flag = False\n",
    "                \n",
    "            save_sim(self.task_name, episode, step, pop_predator, pop_prey, episode_rewards, episode_dones, output_dir=self.model_dir)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN\n",
    "________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUPORTE - Extensivo e Escasso\n",
    "\n",
    "size = 20\n",
    "num_preys = 15\n",
    "num_predators = 10\n",
    "num_episodes = 100\n",
    "perc_obs = 0.05\n",
    "num_steps = 10\n",
    "v = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load - nn_dqn\n",
    "#==========================\n",
    "\n",
    "# Input shape para os modelos\n",
    "input_shape = (7, 7, 3)\n",
    "nn_model = \"nn_dqn\"\n",
    "\n",
    "# Diretório e caminhos para os arquivos salvos\n",
    "directory = \"C:/Users/beLIVE/IA/DECISYS/final-models/last/model\"\n",
    "file_path_model_prey = os.path.join(directory, f'model_{nn_model}_prey.h5')\n",
    "                                               \n",
    "file_path_model_predator = os.path.join(directory, f'model_{nn_model}_predator.h5')\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Presas\n",
    "# ===========================\n",
    "# Criação do modelo de presas\n",
    "model_nn_prey = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de presas (rede principal e alvo)\n",
    "dummy_input = tf.random.uniform((1, *input_shape))\n",
    "model_nn_prey(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_prey.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_prey.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de presas\n",
    "try:\n",
    "    model_nn_prey.load_weights(file_path_model_prey)\n",
    "    print(\"Pesos do modelo de presas carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de presas: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Predadores\n",
    "# ===========================\n",
    "# Criação do modelo de predadores\n",
    "model_nn_predator = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de predadores (rede principal e alvo)\n",
    "model_nn_predator(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_predator.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_predator.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de predadores\n",
    "try:\n",
    "    model_nn_predator.load_weights(file_path_model_predator)\n",
    "    print(\"Pesos do modelo de predadores carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de predadores: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Finalização\n",
    "# ===========================\n",
    "print(\"Modelos prontos para simulação!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RUN - prqipyqi\n",
    "# ==========================================\n",
    "\n",
    "category = \"prqipyqi\"\n",
    "\n",
    "# Carregar os pesos dos modelos aqui, se necessário\n",
    "task_name = f\"sim_{nn_model}{v}_sz{size}_s{num_steps}_py{num_preys}_pd{num_predators}_o{perc_obs}_{category}\"\n",
    "\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "num_obstacle = int(env.sizeX * env.sizeX * perc_obs)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "limit_pop_predator = 0\n",
    "limit_pop_prey = 0\n",
    "\n",
    "# Executando a sessão de teste\n",
    "sim = Sim(task_name, category, env, num_predators, num_preys, limit_pop_predator, limit_pop_prey, model_nn_predator, model_nn_prey, num_episodes, num_steps)\n",
    "\n",
    "sim.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Load nn_per\n",
    "#==========================\n",
    "\n",
    "# Input shape para os modelos\n",
    "input_shape = (7, 7, 3)\n",
    "nn_model = \"nn_per\"\n",
    "\n",
    "\n",
    "# Diretório e caminhos para os arquivos salvos\n",
    "directory = \"C:/Users/beLIVE/IA/DECISYS/final-models/last/model\"\n",
    "file_path_model_prey = os.path.join(directory, f'model_{nn_model}_prey.h5')\n",
    "                                               \n",
    "file_path_model_predator = os.path.join(directory, f'model_{nn_model}_predator.h5')\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Presas\n",
    "# ===========================\n",
    "# Criação do modelo de presas\n",
    "model_nn_prey = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de presas (rede principal e alvo)\n",
    "dummy_input = tf.random.uniform((1, *input_shape))\n",
    "model_nn_prey(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_prey.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_prey.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de presas\n",
    "try:\n",
    "    model_nn_prey.load_weights(file_path_model_prey)\n",
    "    print(\"Pesos do modelo de presas carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de presas: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Predadores\n",
    "# ===========================\n",
    "# Criação do modelo de predadores\n",
    "model_nn_predator = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de predadores (rede principal e alvo)\n",
    "model_nn_predator(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_predator.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_predator.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de predadores\n",
    "try:\n",
    "    model_nn_predator.load_weights(file_path_model_predator)\n",
    "    print(\"Pesos do modelo de predadores carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de predadores: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Finalização\n",
    "# ===========================\n",
    "print(\"Modelos prontos para simulação!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RUN - prqipyqi\n",
    "# ==========================================\n",
    "\n",
    "category = \"prqipyqi\"\n",
    "\n",
    "\n",
    "# Carregar os pesos dos modelos aqui, se necessário\n",
    "task_name = f\"sim_{nn_model}{v}_sz{size}_s{num_steps}_py{num_preys}_pd{num_predators}_o{perc_obs}_{category}\"\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "num_obstacle = int(env.sizeX * env.sizeX * perc_obs)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "limit_pop_predator = 0\n",
    "limit_pop_prey = 0\n",
    "\n",
    "# Executando a sessão de teste\n",
    "sim = Sim(task_name, category, env, num_predators, num_preys, limit_pop_predator, limit_pop_prey, model_nn_predator, model_nn_prey, num_episodes, num_steps)\n",
    "\n",
    "sim.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Load nn_dueling\n",
    "#==========================\n",
    "\n",
    "# Input shape para os modelos\n",
    "input_shape = (7, 7, 3)\n",
    "nn_model = \"nn_dueling\"\n",
    "\n",
    "\n",
    "# Diretório e caminhos para os arquivos salvos\n",
    "directory = \"C:/Users/beLIVE/IA/DECISYS/final-models/last/model\"\n",
    "file_path_model_prey = os.path.join(directory, f'model_{nn_model}_prey.h5')\n",
    "                                               \n",
    "file_path_model_predator = os.path.join(directory, f'model_{nn_model}_predator.h5')\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Presas\n",
    "# ===========================\n",
    "# Criação do modelo de presas\n",
    "model_nn_prey = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de presas (rede principal e alvo)\n",
    "dummy_input = tf.random.uniform((1, *input_shape))\n",
    "model_nn_prey(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_prey.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_prey.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de presas\n",
    "try:\n",
    "    model_nn_prey.load_weights(file_path_model_prey)\n",
    "    print(\"Pesos do modelo de presas carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de presas: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Predadores\n",
    "# ===========================\n",
    "# Criação do modelo de predadores\n",
    "model_nn_predator = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de predadores (rede principal e alvo)\n",
    "model_nn_predator(dummy_input)  # Ativa a rede principal\n",
    "#model_nn_predator.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "#model_nn_predator.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de predadores\n",
    "try:\n",
    "    model_nn_predator.load_weights(file_path_model_predator)\n",
    "    print(\"Pesos do modelo de predadores carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de predadores: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Finalização\n",
    "# ===========================\n",
    "print(\"Modelos prontos para simulação!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. RUN - prqipyqi\n",
    "# ==========================================\n",
    "\n",
    "category = \"prqipyqi\"\n",
    "\n",
    "# Carregar os pesos dos modelos aqui, se necessário\n",
    "task_name = f\"sim_{nn_model}{v}_sz{size}_s{num_steps}_py{num_preys}_pd{num_predators}_o{perc_obs}_{category}\"\n",
    "\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "num_obstacle = int(env.sizeX * env.sizeX * perc_obs)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "limit_pop_predator = 0\n",
    "limit_pop_prey = 0\n",
    "\n",
    "# Executando a sessão de teste\n",
    "sim = Sim(task_name, category, env, num_predators, num_preys, limit_pop_predator, limit_pop_prey, model_nn_predator, model_nn_prey, num_episodes, num_steps)\n",
    "\n",
    "sim.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Load nn_double\n",
    "#==========================\n",
    "\n",
    "# Input shape para os modelos\n",
    "input_shape = (7, 7, 3)\n",
    "nn_model = \"nn_double\"\n",
    "\n",
    "\n",
    "# Diretório e caminhos para os arquivos salvos\n",
    "directory = \"C:/Users/beLIVE/IA/DECISYS/final-models/last/model\"\n",
    "file_path_model_prey = os.path.join(directory, f'model_{nn_model}_prey.h5')\n",
    "                                               \n",
    "file_path_model_predator = os.path.join(directory, f'model_{nn_model}_predator.h5')\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Presas\n",
    "# ===========================\n",
    "# Criação do modelo de presas\n",
    "model_nn_prey = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de presas (rede principal e alvo)\n",
    "dummy_input = tf.random.uniform((1, *input_shape))\n",
    "model_nn_prey(dummy_input)  # Ativa a rede principal\n",
    "model_nn_prey.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "model_nn_prey.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de presas\n",
    "try:\n",
    "    model_nn_prey.load_weights(file_path_model_prey)\n",
    "    print(\"Pesos do modelo de presas carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de presas: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Configuração do modelo de Predadores\n",
    "# ===========================\n",
    "# Criação do modelo de predadores\n",
    "model_nn_predator = globals()[nn_model](num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Inicialização do modelo de predadores (rede principal e alvo)\n",
    "model_nn_predator(dummy_input)  # Ativa a rede principal\n",
    "model_nn_predator.target_call(dummy_input)  # Ativa a rede-alvo\n",
    "model_nn_predator.update_target_network()  # Cria e sincroniza a rede-alvo\n",
    "\n",
    "# Carregamento dos pesos para o modelo de predadores\n",
    "try:\n",
    "    model_nn_predator.load_weights(file_path_model_predator)\n",
    "    print(\"Pesos do modelo de predadores carregados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os pesos do modelo de predadores: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# Finalização\n",
    "# ===========================\n",
    "print(\"Modelos prontos para simulação!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. RUN - prqipyqi\n",
    "# ==========================================\n",
    "\n",
    "category = \"prqipyqi\"\n",
    "\n",
    "# Carregar os pesos dos modelos aqui, se necessário\n",
    "task_name = f\"sim_{nn_model}{v}_sz{size}_s{num_steps}_py{num_preys}_pd{num_predators}_o{perc_obs}_{category}\"\n",
    "\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "num_obstacle = int(env.sizeX * env.sizeX * perc_obs)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "limit_pop_predator = 0\n",
    "limit_pop_prey = 0\n",
    "\n",
    "# Executando a sessão de teste\n",
    "sim = Sim(task_name, category, env, num_predators, num_preys, limit_pop_predator, limit_pop_prey, model_nn_predator, model_nn_prey, num_episodes, num_steps)\n",
    "\n",
    "sim.run()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "p38-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
