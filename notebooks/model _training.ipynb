{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAÇÕES\n",
    "# ==========================================\n",
    "\n",
    "# Bibliotecas para manipulação do sistema e ambiente de aprendizado por reforço\n",
    "import os\n",
    "\n",
    "# Bibliotecas para operações matemáticas e aleatoriedade\n",
    "import numpy as np\n",
    "import random\n",
    "import logging  # Para registro de mensagens e depuração\n",
    "import itertools  # Para geração de combinações e permutações\n",
    "import statistics\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "# Biblioteca para manipulações de DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca para operações de Deep Learning com TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, InputLayer, MultiHeadAttention, LayerNormalization, Reshape, MultiHeadAttention, GlobalAveragePooling2D, TimeDistributed, MaxPooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam  # Otimizador para ajuste dos pesos da rede neural\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber\n",
    "from tensorflow.keras.regularizers import l2  # Regularização L2 para controle de overfitting\n",
    "from tensorflow.keras.models import load_model, Model  # Carregar modelos pré-treinados, \n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import (InputLayer, Conv2D, BatchNormalization, Flatten, \n",
    "                                     Dense, Dropout, Activation, Concatenate, Multiply, Layer)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "# Verificar dispositivos físicos disponíveis (por exemplo, GPU)\n",
    "#device_lib.list_local_devices()\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: Agents\n",
    "# ==========================================\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, x, y, env, breed, channel):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.env = env\n",
    "        self.breed = breed\n",
    "        self.channel = channel\n",
    "        self.is_alive = True\n",
    "        self.is_done = False\n",
    "        self.current_target = None\n",
    "        self.current_ally = None\n",
    "        self.last_coord = self.x, self.y\n",
    "        self.kid = False\n",
    "\n",
    "        # Category         | Breed | Color  |     Channel      | Color - Att | Channel - Att\n",
    "        # ---------------------------------------------------------------------------\n",
    "        # Predator         |   0   | Red    |    [1, 0, 0]     |   Yellow     | [1, 1, 0]\n",
    "        # Vegetation       |   1   | Green  |    [0, 1, 0]     |  --------    | ---------\n",
    "        # Prey             |   2   | Blue   |    [0, 0, 1]     |   Cyan       | [0, 1, 1]\n",
    "        # Outside Grid     |   -   | White  |    [1, 1, 1]     |  --------    | ---------\n",
    "        # Empty Grid Cell  |   -   | Black  |    [0, 0, 0]     |  --------    | ---------\n",
    "        # Reserved Training|   -   | Gray   | [0.5, 0.5, 0.5]  |  --------    | ---------\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.x, self.y = env.new_position()\n",
    "        self.is_alive = True\n",
    "        self.target = None\n",
    "        self.is_done = False\n",
    "        self.current_target = None\n",
    "        self.current_ally = None\n",
    "        \n",
    "        # Restaurar o `channel` para a cor padrão com base no `breed`\n",
    "        if self.breed == 0:  # Predador\n",
    "            self.channel = [1.0, 0.0, 0.0]\n",
    "        elif self.breed == 2:  # Presa\n",
    "            self.channel = [0.0, 0.0, 1.0]\n",
    "\n",
    "    def step(self, action):\n",
    "        penalty = self.env.move_agent(self, action)\n",
    "        done, reward, ler = self.check_goal()\n",
    "        state = self.env.render_agent(self)\n",
    "\n",
    "        return state, reward + penalty, done, ler\n",
    "    \n",
    "class Prey(Agent):\n",
    "    def __init__(self, x, y, env, id, nn = None, is_on = True):\n",
    "        super().__init__(x, y, env, breed=2, channel=[0.0, 0.0, 1.0])\n",
    "        self.name = f\"Prey_{id}\"\n",
    "        self.in_danger = False\n",
    "        self.is_on = is_on\n",
    "        self.model_nn = nn\n",
    "\n",
    "    def check_goal(self):\n",
    "        done = False\n",
    "        reward = 5.0  # Recompensa padrão para incentivar a exploração segura.\n",
    "        feedback = \"\"\n",
    "        target_detected = False\n",
    "        ally_detected = False\n",
    "\n",
    "        closest_target_distance = float('inf')\n",
    "        closest_ally_distance = float('inf')\n",
    "\n",
    "        # Procura por agente vivo, presa ou predador, mais próxima ou mantém o foco na atual.\n",
    "        for agent in self.env.agents:\n",
    "            # Verifica se é predador\n",
    "            if agent.breed == 0 and agent.is_alive:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_target is None or distance < closest_target_distance):\n",
    "                    closest_target_distance = distance\n",
    "                    self.current_target = agent\n",
    "                    target_detected = True\n",
    "\n",
    "            # Verifica se é presa com informação de predador\n",
    "            elif agent.breed == 2 and agent.is_alive and agent.channel == [0, 1, 1]:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_ally is None or distance < closest_ally_distance):\n",
    "                    closest_ally_distance = distance\n",
    "                    self.current_ally = agent\n",
    "                    ally_detected = True\n",
    "\n",
    "        \n",
    "        # Verificação de fuga do predador\n",
    "        if self.in_danger and closest_target_distance > 3:\n",
    "            done = True\n",
    "            self.in_danger = False\n",
    "            reward += 20.0  # Recompensa máxima por escapar do perigo\n",
    "            feedback = f\"[PREY]: Evasão do alvo\"\n",
    "        \n",
    "        # Critérios de Recompensa/Penalidade\n",
    "        elif target_detected and 0 < closest_target_distance <= 3:\n",
    "            # Penalidade por proximidade com predador\n",
    "            self.in_danger = True\n",
    "            if closest_target_distance == 3:\n",
    "                reward -= 1.0\n",
    "            elif closest_target_distance == 2:\n",
    "                reward -= 3.0\n",
    "            elif closest_target_distance == 1:\n",
    "                reward -= 5.0\n",
    "            feedback = f\"[PREY]: Proximidade predador: {closest_target_distance}\"\n",
    "\n",
    "        elif ally_detected and not target_detected and 0 < closest_ally_distance <= 3:\n",
    "            # Recompensa por proximidade com aliado (se não houver predador no campo de visão)\n",
    "            if closest_ally_distance == 3:\n",
    "                reward -= 0.1\n",
    "            elif closest_ally_distance == 2:\n",
    "                reward -= 0.3\n",
    "            elif closest_ally_distance == 1:\n",
    "                reward -= 0.5\n",
    "            feedback += f\"[PREY]: Proximidade aliado: {closest_ally_distance}\"\n",
    "\n",
    "        else:\n",
    "            feedback += \"[PREY]: Explorando mapa\"\n",
    "\n",
    "\n",
    "        # Atualiza channel\n",
    "        for agent in self.env.agents:\n",
    "            if agent.is_alive and agent.breed == 0:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray:\n",
    "                    self.channel = [0.0, 1.0, 1.0]\n",
    "                    break\n",
    "                else:\n",
    "                    self.channel = [0.0, 0.0, 1.0]\n",
    "\n",
    "        return done, reward, feedback\n",
    "\n",
    "class Predator(Agent):\n",
    "    def __init__(self, x, y, env, id, nn = None, is_on = True):\n",
    "        super().__init__(x, y, env, breed=0, channel=[1.0, 0.0, 0.0])\n",
    "        self.name = f\"Predador_{id}\"\n",
    "        self.last_hunt = None\n",
    "        self.is_on = is_on\n",
    "\n",
    "    def check_goal(self):\n",
    "        done = False\n",
    "        reward = -0.05  # Penalidade leve para incentivar movimentação\n",
    "        feedback = \"\"\n",
    "        target_detected = False\n",
    "        ally_detected = False\n",
    "\n",
    "        closest_target_distance = float('inf')\n",
    "        closest_ally_distance = float('inf')\n",
    "\n",
    "        # Procura por agente vivo, presa ou predador, mais próxima ou mantém o foco na atual.\n",
    "        for agent in self.env.agents:\n",
    "            # Verifica se é presa\n",
    "            if agent.is_alive and agent.breed == 2:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_target is None or distance < closest_target_distance):\n",
    "                    closest_target_distance = distance\n",
    "                    self.current_target = agent\n",
    "                    target_detected = True\n",
    "\n",
    "            # Verifica se é predador com informação de presa\n",
    "            elif agent.breed == 0 and agent.is_alive and agent.channel == [1.0, 1.0, 0.0]:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray and (self.current_ally is None or distance < closest_ally_distance):\n",
    "                    closest_ally_distance = distance\n",
    "                    self.current_ally = agent\n",
    "                    ally_detected = True\n",
    "\n",
    "        # Critérios de Recompensa\n",
    "        if target_detected and closest_target_distance == 0:\n",
    "            done = True\n",
    "            reward += 20.0  # Recompensa máxima por capturar a presa\n",
    "            self.current_target.is_alive = False\n",
    "            feedback = f\"[PREDATOR]: Alvo capturado\"\n",
    "            # remover da lista esta na classe de treinamento\n",
    "            self.current_target = None\n",
    "\n",
    "        elif target_detected and 0 < closest_target_distance <= 3:\n",
    "            # Recompensa por proximidade com a presa\n",
    "            if closest_target_distance == 3:\n",
    "                reward += 0.5\n",
    "            elif closest_target_distance == 2:\n",
    "                reward += 1.0\n",
    "            elif closest_target_distance == 1:\n",
    "                reward += 2.0\n",
    "            feedback = f\"[PREDATOR]: Proximidade presa: {closest_target_distance}\"\n",
    "\n",
    "        elif ally_detected and 0 < closest_ally_distance <= 3:\n",
    "            # Recompensa por proximidade com aliado (que tem informação de presa)\n",
    "            if closest_ally_distance == 3:\n",
    "                reward += 0.1\n",
    "            elif closest_ally_distance == 2:\n",
    "                reward += 0.3\n",
    "            elif closest_ally_distance == 1:\n",
    "                reward += 0.5\n",
    "            feedback = f\"[PREDATOR]: Proximidade aliado: {closest_ally_distance}\"\n",
    "\n",
    "        else:\n",
    "            feedback += \"[PREDATOR]: Explorando mapa\"\n",
    "\n",
    "        # Atualiza channel\n",
    "        for agent in self.env.agents:\n",
    "            if agent.is_alive and agent.breed == 2:\n",
    "                distance = self.env.chebyshev_distance(self.x, self.y, agent.x, agent.y)\n",
    "                if distance <= self.env.ray:\n",
    "                    self.channel = [1.0, 1.0, 0.0]\n",
    "                    break\n",
    "                else:\n",
    "                    self.channel = [1.0, 0.0, 0.0]\n",
    "\n",
    "        return done, reward, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: Obstacle\n",
    "# ==========================================\n",
    "\n",
    "class Obstacle:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.channel = [0.0, 1.0, 0.0]  # Cor verde para o obstáculo (RGB)\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.channel = [0.0, 1.0, 0.0]\n",
    "\n",
    "        # Escolhe uma posição aleatória no grid que esteja livre\n",
    "        position = env.new_position()\n",
    "        if position:\n",
    "            self.x, self.y = position\n",
    "        else:\n",
    "            raise ValueError(\"Não foi possível encontrar uma posição livre para o obstáculo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: Environment\n",
    "# ==========================================\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, sizeX, sizeY, ray=3):\n",
    "        self.sizeX = sizeX\n",
    "        self.sizeY = sizeY\n",
    "        self.ray = ray\n",
    "        self.agents = []\n",
    "        self.objects = []\n",
    "        self.obstacles = []  # Lista separada para obstáculos\n",
    "        self.actions = 9  # Número de ações possíveis\n",
    "\n",
    "\n",
    "    def new_position(self):\n",
    "        # Cria uma lista de todas as posições possíveis.\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = list(itertools.product(*iterables))\n",
    "\n",
    "        # Cria uma lista das posições atuais ocupadas pelos agentes e obstáculos.\n",
    "        current_positions = [(agent.x, agent.y) for agent in self.agents]\n",
    "        current_positions += [(obstacle.x, obstacle.y) for obstacle in self.obstacles]\n",
    "\n",
    "        # Filtra as posições possíveis, removendo as ocupadas.\n",
    "        available_points = [point for point in points if point not in current_positions]\n",
    "\n",
    "        # Escolhe aleatoriamente uma das posições disponíveis.\n",
    "        if available_points:\n",
    "            return random.choice(available_points)\n",
    "        else:\n",
    "            # Loga a mensagem indicando que não há posições disponíveis.\n",
    "            logging.info(\"Não foi possível encontrar uma nova posição disponível.\")\n",
    "            return None\n",
    "\n",
    "    def add_obstacle(self, obstacle):\n",
    "        # Define uma posição disponível para o obstáculo\n",
    "        position = self.new_position()\n",
    "        if position:\n",
    "            obstacle.x, obstacle.y = position  # Atribui as coordenadas ao obstáculo\n",
    "            self.objects.append(obstacle)\n",
    "        else:\n",
    "            logging.info(\"Não foi possível adicionar um novo obstáculo: nenhuma posição disponível.\")\n",
    "\n",
    "    def add_agent(self, agent):\n",
    "        # Define uma posição disponível para o agente\n",
    "        position = self.new_position()\n",
    "        if position:\n",
    "            agent.x, agent.y = position  # Atribui as coordenadas ao agente\n",
    "            self.objects.append(agent)\n",
    "        else:\n",
    "            logging.info(\"Não foi possível adicionar um novo agente: nenhuma posição disponível.\")\n",
    "\n",
    "    def reset(self):\n",
    "        # Verifica se a lista de objetos está vazia\n",
    "        if not self.objects:\n",
    "            logging.info(\"Não é possível prosseguir: nenhum objeto foi adicionado ao ambiente.\")\n",
    "            return  # Interrompe o método se `self.objects` estiver vazio\n",
    "\n",
    "        # Limpa as listas de agentes e obstáculos para reiniciar o ambiente\n",
    "        self.agents = []\n",
    "        self.obstacles = []\n",
    "\n",
    "        # Embaralha a lista de objetos para variar a ordem dos elementos no ambiente\n",
    "        random.shuffle(self.objects)\n",
    "\n",
    "        # Adiciona os agentes e obstáculos à lista apropriada e os reseta\n",
    "        for item in self.objects:\n",
    "            item.reset(self)  # Reposiciona cada objeto no ambiente\n",
    "            if isinstance(item, Agent):  # Considerando uma classe Agent\n",
    "                self.agents.append(item)\n",
    "            elif isinstance(item, Obstacle):  # Considerando uma classe Obstacle\n",
    "                self.obstacles.append(item)\n",
    "\n",
    "    def is_position_empty_and_valid(self, x, y):\n",
    "        # Verifica se a posição está dentro dos limites do ambiente\n",
    "        if x < 0 or x >= self.sizeX or y < 0 or y >= self.sizeY:\n",
    "            return False  # A posição está fora dos limites do ambiente\n",
    "\n",
    "        # Verifica se a posição está ocupada por algum agente\n",
    "        for agent in self.agents:\n",
    "            if agent.x == x and agent.y == y:\n",
    "                return False  # A posição está ocupada\n",
    "\n",
    "        # Verifica se a posição está ocupada por algum obstáculo\n",
    "        for obstacle in self.obstacles:\n",
    "            if obstacle.x == x and obstacle.y == y:\n",
    "                return False  # A posição está ocupada por um obstáculo\n",
    "\n",
    "    def get_agent_at_position(self, x, y):\n",
    "        for agent in self.agents:\n",
    "            if agent.x == x and agent.y == y:\n",
    "                return agent\n",
    "        return None\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Inicializa a penalidade padrão e os valores de penalidade\n",
    "        ZERO = 0.0\n",
    "        PENALIZE = -10.0\n",
    "        direction = action\n",
    "\n",
    "        # Inicializa os incrementos de movimento\n",
    "        new_x, new_y = 0, 0\n",
    "\n",
    "        # Define os incrementos de movimento com base na direção\n",
    "        if direction == 0:  # Para cima\n",
    "            new_y = -1\n",
    "        elif direction == 1:  # Para cima e direita (diagonal)\n",
    "            new_x = 1\n",
    "            new_y = -1\n",
    "        elif direction == 2:  # Para direita\n",
    "            new_x = 1\n",
    "        elif direction == 3:  # Para baixo e direita (diagonal)\n",
    "            new_x = 1\n",
    "            new_y = 1\n",
    "        elif direction == 4:  # Para baixo\n",
    "            new_y = 1\n",
    "        elif direction == 5:  # Para baixo e esquerda (diagonal)\n",
    "            new_x = -1\n",
    "            new_y = 1\n",
    "        elif direction == 6:  # Para esquerda\n",
    "            new_x = -1\n",
    "        elif direction == 7:  # Para cima e esquerda (diagonal)\n",
    "            new_x = -1\n",
    "            new_y = -1\n",
    "        elif direction == 8:  # Ficar parado\n",
    "            new_x = 0\n",
    "            new_y = 0\n",
    "\n",
    "        # Calcula a nova posição absoluta do agente\n",
    "        target_x = agent.x + new_x\n",
    "        target_y = agent.y + new_y\n",
    "\n",
    "        # Verifica se a nova posição contém um obstáculo\n",
    "        if any(obstacle.x == target_x and obstacle.y == target_y for obstacle in self.obstacles):\n",
    "            print(\"Agente tentou ocupar um obstáculo!\")\n",
    "            return PENALIZE  # Penalidade por tentar ocupar a posição de um obstáculo\n",
    "\n",
    "        # Verifica se o movimento está dentro dos limites do ambiente\n",
    "        if target_x < 0 or target_x >= self.sizeX or target_y < 0 or target_y >= self.sizeY:\n",
    "            print(\"Agente fora do limite!\")\n",
    "            return PENALIZE\n",
    "\n",
    "        # Verifica se a nova posição contém outro agente\n",
    "        other_agent = self.get_agent_at_position(target_x, target_y)\n",
    "        if other_agent:\n",
    "            # Lógica para a presa\n",
    "            if agent.breed == 2:  # Presa\n",
    "                print(\"Presa não pode ocupar a posição de outro agente!\")\n",
    "                return PENALIZE  # Penalidade por tentar ocupar a posição de outro agente\n",
    "\n",
    "            # Lógica para o predador\n",
    "            elif agent.breed == 0:  # Predador\n",
    "                if other_agent.breed == 0:  # Outro predador na posição\n",
    "                    print(\"Predador não pode ocupar a posição de outro predador!\")\n",
    "                    return PENALIZE  # Penalidade por tentar ocupar a posição de outro predador\n",
    "\n",
    "        # Atualiza a posição do agente se o movimento for válido\n",
    "        agent.x, agent.y = target_x, target_y\n",
    "        return ZERO\n",
    "\n",
    "    def render_env(self):\n",
    "        a = np.zeros([self.sizeY, self.sizeX, 3])\n",
    "\n",
    "        for agent in self.agents:\n",
    "            if agent.x is not None and agent.y is not None:\n",
    "                a[agent.y, agent.x, :] = agent.channel\n",
    "\n",
    "        for obstacle in self.obstacles:\n",
    "            a[obstacle.y, obstacle.x, :] = obstacle.channel  # Renderiza obstáculos\n",
    "\n",
    "        return a\n",
    "\n",
    "    def render_agent(self, agent):\n",
    "        # Renderiza o ambiente para obter a matriz RGB atual\n",
    "        a = self.render_env()\n",
    "\n",
    "        # Calcula o tamanho do recorte com base em self.ray\n",
    "        recorte_tamanho = 2 * self.ray + 1\n",
    "\n",
    "        # Inicializa o recorte temporário com cor amarela\n",
    "        recorte_temp = np.ones((recorte_tamanho, recorte_tamanho, 3)) * np.array([1.0, 1.0, 1.0])  # fora do Grid - GRAY\n",
    "\n",
    "        # Calcula as coordenadas do recorte dentro do ambiente\n",
    "        inicio_x = agent.x - self.ray\n",
    "        inicio_y = agent.y - self.ray\n",
    "        fim_x = inicio_x + recorte_tamanho\n",
    "        fim_y = inicio_y + recorte_tamanho\n",
    "\n",
    "        # Calcula os limites de sobreposição entre o recorte e o ambiente\n",
    "        sobreposicao_inicio_x = max(inicio_x, 0)\n",
    "        sobreposicao_inicio_y = max(inicio_y, 0)\n",
    "        sobreposicao_fim_x = min(fim_x, self.sizeX)\n",
    "        sobreposicao_fim_y = min(fim_y, self.sizeY)\n",
    "\n",
    "        # Calcula os índices de destino no recorte temporário\n",
    "        destino_inicio_x = sobreposicao_inicio_x - inicio_x\n",
    "        destino_inicio_y = sobreposicao_inicio_y - inicio_y\n",
    "        destino_fim_x = destino_inicio_x + sobreposicao_fim_x - sobreposicao_inicio_x\n",
    "        destino_fim_y = destino_inicio_y + sobreposicao_fim_y - sobreposicao_inicio_y\n",
    "\n",
    "        # Copia a sobreposição do ambiente para o recorte temporário\n",
    "        recorte_temp[destino_inicio_y:destino_fim_y, destino_inicio_x:destino_fim_x] = \\\n",
    "            a[sobreposicao_inicio_y:sobreposicao_fim_y, sobreposicao_inicio_x:sobreposicao_fim_x]\n",
    "\n",
    "        # Pinta o elemento central do recorte de branco, ajustando a posição baseada em self.ray\n",
    "        centro = self.ray\n",
    "        recorte_temp[centro, centro, :] = np.array([0.5, 0.5, 0.5])  # Cinza - Destaque de Agente em treinamento\n",
    "\n",
    "        return recorte_temp\n",
    "\n",
    "    def population_count(self):\n",
    "        \"\"\"Retorna a quantidade de presas, predadores e obstáculos no ambiente.\"\"\"\n",
    "        predator_count = 0\n",
    "        prey_count = 0\n",
    "\n",
    "        for agent in self.agents:\n",
    "            if agent.is_alive is True:\n",
    "                if agent.breed == 0:\n",
    "                    predator_count += 1\n",
    "                elif agent.breed == 2:\n",
    "                    prey_count += 1\n",
    "\n",
    "        obstacle_count = len(self.obstacles)  # Conta o total de obstáculos na lista `self.obstacles`\n",
    "        return prey_count, predator_count, obstacle_count\n",
    "\n",
    "    def remove_agent(self, agent):\n",
    "        self.agents.remove(agent)\n",
    "\n",
    "    @staticmethod\n",
    "    def chebyshev_distance(x1, y1, x2, y2):\n",
    "        distance = max(abs(x2 - x1), abs(y2 - y1))\n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: PrioritizedReplayBuffer\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.pos = 0\n",
    "\n",
    "    def add(self, transition, td_error):\n",
    "        \"\"\"Adiciona uma transição ao buffer com prioridade baseada no erro TD.\"\"\"\n",
    "        max_priority = max(self.priorities, default=1.0)  # Prioridade máxima inicial\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "            self.priorities.append(max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "            self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"Amostra um minibatch com base nas prioridades.\"\"\"\n",
    "        priorities = np.array(self.priorities)\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # Calcular pesos de importância\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Atualiza as prioridades com base nos novos erros TD.\"\"\"\n",
    "        for i, td_error in zip(indices, td_errors):\n",
    "            self.priorities[i] = abs(td_error) + 1e-5  # Adiciona pequeno valor para estabilidade numérica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support\n",
    "# ==========================================\n",
    "\n",
    "lr = 0.0001\n",
    "l2_regularization = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class - RADAR\n",
    "# ==========================================\n",
    "\n",
    "class ColorCombDepthwiseConv2D(Layer):\n",
    "    def __init__(self, kernel_size=(7, 7), activation='relu', padding='same', **kwargs):\n",
    "        super(ColorCombDepthwiseConv2D, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.padding = padding\n",
    "\n",
    "        # Convoluções individuais para cores puras\n",
    "        self.conv_r = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_r\")\n",
    "        self.bn_r = BatchNormalization(name=\"bn_r\")\n",
    "        \n",
    "        self.conv_g = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_g\")\n",
    "        self.bn_g = BatchNormalization(name=\"bn_g\")\n",
    "        \n",
    "        self.conv_b = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_b\")\n",
    "        self.bn_b = BatchNormalization(name=\"bn_b\")\n",
    "\n",
    "        # Convoluções para combinações específicas (Magenta, Ciano, Amarelo)\n",
    "        self.conv_magenta = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_magenta\")\n",
    "        self.bn_magenta = BatchNormalization(name=\"bn_magenta\")\n",
    "        \n",
    "        self.conv_cyan = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_cyan\")\n",
    "        self.bn_cyan = BatchNormalization(name=\"bn_cyan\")\n",
    "        \n",
    "        self.conv_yellow = Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, activation=None, name=\"conv_yellow\")\n",
    "        self.bn_yellow = BatchNormalization(name=\"bn_yellow\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        r, g, b = tf.split(inputs, num_or_size_splits=3, axis=-1)\n",
    "\n",
    "        # Convoluções individuais com BatchNormalization\n",
    "        r_out = self.bn_r(self.conv_r(r), training=training)\n",
    "        g_out = self.bn_g(self.conv_g(g), training=training)\n",
    "        b_out = self.bn_b(self.conv_b(b), training=training)\n",
    "\n",
    "        # Combinações de canais com BatchNormalization\n",
    "        magenta_out = self.bn_magenta(self.conv_magenta(r + b), training=training)\n",
    "        cyan_out = self.bn_cyan(self.conv_cyan(g + b), training=training)\n",
    "        yellow_out = self.bn_yellow(self.conv_yellow(r + g), training=training)\n",
    "\n",
    "        # Concatenando saídas\n",
    "        outputs = Concatenate(axis=-1, name=\"concat_colors\")([r_out, g_out, b_out, magenta_out, cyan_out, yellow_out])\n",
    "\n",
    "        # Ativação\n",
    "        outputs = Activation(self.activation, name=\"activation_colors\")(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SpatialAttentionModule(Layer):\n",
    "    def __init__(self, kernel_size=4):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv = Conv2D(1, kernel_size=kernel_size, padding='same', activation=None, name=\"attention_conv\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        squared_inputs = K.square(inputs)\n",
    "        l2_pool = tf.sqrt(tf.reduce_mean(squared_inputs, axis=-1, keepdims=True) + 1e-6)\n",
    "\n",
    "        concat = Concatenate(axis=-1, name=\"concat_attention\")([avg_pool, l2_pool])\n",
    "        attention_map = self.conv(concat)\n",
    "        attention_map = Activation('sigmoid', name=\"sigmoid_attention\")(attention_map)\n",
    "        return Multiply(name=\"apply_attention\")([inputs, attention_map])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_dqn\n",
    "# ==========================================\n",
    "\n",
    "class nn_dqn(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_dqn, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camadas convolucionais\n",
    "        self.input_layer = InputLayer(input_shape=input_shape)\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout após conv1\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout após conv2\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.5, name=\"dropout_dense1_layer\")  # Dropout após dense1\n",
    "        \n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.5, name=\"dropout_dense2_layer\")  # Dropout após dense2\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado após conv1\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado após conv2\n",
    "\n",
    "        # Atenção espacial e Flatten\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas densas\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Dropout aplicado após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Dropout aplicado após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_per\n",
    "# ==========================================\n",
    "\n",
    "class nn_per(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_per, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.3, name=\"dropout_flatten_layer\")\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.4, name=\"dropout_dense1_layer\")\n",
    "\n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.4, name=\"dropout_dense2_layer\")\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Aplicação do Dropout após conv1\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Aplicação do Dropout após conv2\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Aplicação do Dropout após Flatten\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Aplicação do Dropout após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Aplicação do Dropout após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ, weights = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            td_errors = targetQ - Q\n",
    "            weighted_loss = tf.reduce_mean(weights * tf.square(td_errors))\n",
    "\n",
    "        grads = tape.gradient(weighted_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return weighted_loss  # Apenas a perda\n",
    "\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_dueling\n",
    "# ==========================================\n",
    "\n",
    "class nn_dueling(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_dueling, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "        \n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout na primeira camada convolucional\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout na segunda camada convolucional\n",
    "\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        # Camadas densas compartilhadas\n",
    "        self.dense_shared1 = Dense(64, activation='relu', name=\"shared_dense1\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_shared = Dropout(0.4, name=\"dropout_shared_layer\")  # Dropout na camada compartilhada\n",
    "\n",
    "        # Rede para Valor (V)\n",
    "        self.value_dense = Dense(32, activation='relu', name=\"value_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_value = Dropout(0.4, name=\"dropout_value_layer\")  # Dropout na rede de valor\n",
    "        self.value_output = Dense(1, activation='linear', name=\"value_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Rede para Vantagem (A)\n",
    "        self.advantage_dense = Dense(32, activation='relu', name=\"advantage_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_advantage = Dropout(0.4, name=\"dropout_advantage_layer\")  # Dropout na rede de vantagem\n",
    "        self.advantage_output = Dense(num_actions, activation='linear', name=\"advantage_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado\n",
    "        \n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas compartilhadas\n",
    "        x = self.dense_shared1(x)\n",
    "        x = self.dropout_shared(x, training=training)  # Dropout aplicado na camada compartilhada\n",
    "\n",
    "        # Valor (V)\n",
    "        v = self.value_dense(x)\n",
    "        v = self.dropout_value(v, training=training)  # Dropout aplicado na rede de valor\n",
    "        v = self.value_output(v)\n",
    "\n",
    "        # Vantagem (A)\n",
    "        a = self.advantage_dense(x)\n",
    "        a = self.dropout_advantage(a, training=training)  # Dropout aplicado na rede de vantagem\n",
    "        a = self.advantage_output(a)\n",
    "\n",
    "        # Combina V e A para calcular Q\n",
    "        q = v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return q\n",
    "    \n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_double\n",
    "# ==========================================\n",
    "\n",
    "class nn_double(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_double, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Otimizador e função de perda\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Camada de entrada e processamento inicial de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "\n",
    "        # Camadas convolucionais da rede principal\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "\n",
    "        # Camadas densas\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Camadas da rede-alvo (adicionadas as mesmas modificações)\n",
    "        self.target_conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm1 = BatchNormalization()\n",
    "        self.target_relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm2 = BatchNormalization()\n",
    "        self.target_relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "        self.target_flatten = Flatten()\n",
    "        self.target_dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede principal para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def target_call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede-alvo para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        \n",
    "\n",
    "        x = self.target_conv1(x)\n",
    "        x = self.target_batch_norm1(x, training=training)\n",
    "        x = self.target_relu1(x)\n",
    "\n",
    "        x = self.target_conv2(x)\n",
    "        x = self.target_batch_norm2(x, training=training)\n",
    "        x = self.target_relu2(x)\n",
    "\n",
    "        \n",
    "        x = self.target_flatten(x)\n",
    "        x = self.target_dense1(x)\n",
    "\n",
    "        x = self.target_dense2(x)\n",
    "        return self.target_output_layer(x)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Sincroniza os pesos da rede principal para a rede-alvo.\"\"\"\n",
    "        self.target_conv1.set_weights(self.conv1.get_weights())\n",
    "        self.target_batch_norm1.set_weights(self.batch_norm1.get_weights())\n",
    "        self.target_conv2.set_weights(self.conv2.get_weights())\n",
    "        self.target_batch_norm2.set_weights(self.batch_norm2.get_weights())\n",
    "        self.target_dense1.set_weights(self.dense1.get_weights())\n",
    "        self.target_dense2.set_weights(self.dense2.get_weights())\n",
    "        self.target_output_layer.set_weights(self.output_layer.get_weights())\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        \"\"\"Realiza uma etapa de treinamento com Double DQN.\"\"\"\n",
    "        states, actions, targetQ = batch_data\n",
    "        next_states, rewards, dones = targetQ\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predições da rede principal\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "\n",
    "            # Double DQN: Calcula o valor-alvo\n",
    "            main_Q_values_next = self(next_states)  # Rede principal para selecionar a melhor ação\n",
    "            next_actions = tf.argmax(main_Q_values_next, axis=1)\n",
    "            target_Q_values_next = self.target_call(next_states)  # Rede-alvo para calcular Q\n",
    "            target_Q = tf.reduce_sum(target_Q_values_next * tf.one_hot(next_actions, self.num_actions), axis=1)\n",
    "            target_Q = rewards + (1 - dones) * self.discount_factor * target_Q\n",
    "\n",
    "            # Calcula o loss\n",
    "            loss = self.loss_fn(target_Q, Q)\n",
    "\n",
    "        # Gradientes e atualização dos pesos\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Prediz a melhor ação para um dado estado.\"\"\"\n",
    "        Q_values = self(tf.expand_dims(state, axis=0))\n",
    "        return tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"Salva os pesos do modelo principal.\"\"\"\n",
    "        self.save_weights(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"Carrega os pesos do modelo principal.\"\"\"\n",
    "        self.load_weights(file_path)\n",
    "        print(f\"Modelo carregado de: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_dqn\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_dqn(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_radar_dqn, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camadas convolucionais\n",
    "        self.input_layer = InputLayer(input_shape=input_shape)\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout após conv1\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout após conv2\n",
    "\n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.5, name=\"dropout_dense1_layer\")  # Dropout após dense1\n",
    "        \n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.5, name=\"dropout_dense2_layer\")  # Dropout após dense2\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado após conv1\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado após conv2\n",
    "\n",
    "        # Atenção espacial e Flatten\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas densas\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Dropout aplicado após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Dropout aplicado após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_per\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_per(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_radar_per, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")\n",
    "        \n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten e camadas densas\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.3, name=\"dropout_flatten_layer\")\n",
    "\n",
    "        self.dense1 = Dense(64, activation='relu', name=\"dense1_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense1 = Dropout(0.4, name=\"dropout_dense1_layer\")\n",
    "\n",
    "        self.dense2 = Dense(32, activation='relu', name=\"dense2_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_dense2 = Dropout(0.4, name=\"dropout_dense2_layer\")\n",
    "\n",
    "        self.dense_output = Dense(num_actions, activation='linear', name=\"dense_output_layer\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Aplicação do Dropout após conv1\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Aplicação do Dropout após conv2\n",
    "\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Aplicação do Dropout após Flatten\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout_dense1(x, training=training)  # Aplicação do Dropout após dense1\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_dense2(x, training=training)  # Aplicação do Dropout após dense2\n",
    "\n",
    "        Q_values = self.dense_output(x)\n",
    "        return Q_values\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ, weights = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            td_errors = targetQ - Q\n",
    "            weighted_loss = tf.reduce_mean(weights * tf.square(td_errors))\n",
    "\n",
    "        grads = tape.gradient(weighted_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return weighted_loss  # Apenas a perda\n",
    "\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_dueling\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_dueling(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3)):\n",
    "        super(nn_radar_dueling, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "        \n",
    "        # Entrada e camada de combinação de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding='same', name=\"conv1_layer\")\n",
    "        self.bn1 = BatchNormalization(name=\"bn1_layer\")\n",
    "        self.dropout_conv1 = Dropout(0.3, name=\"dropout_conv1_layer\")  # Dropout na primeira camada convolucional\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding='same', name=\"conv2_layer\")\n",
    "        self.bn2 = BatchNormalization(name=\"bn2_layer\")\n",
    "        self.dropout_conv2 = Dropout(0.3, name=\"dropout_conv2_layer\")  # Dropout na segunda camada convolucional\n",
    "     \n",
    "        # Camada de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Flatten\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "        self.dropout_flatten = Dropout(0.4, name=\"dropout_flatten_layer\")  # Dropout após Flatten\n",
    "\n",
    "        # Camadas densas compartilhadas\n",
    "        self.dense_shared1 = Dense(64, activation='relu', name=\"shared_dense1\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_shared = Dropout(0.4, name=\"dropout_shared_layer\")  # Dropout na camada compartilhada\n",
    "\n",
    "        # Rede para Valor (V)\n",
    "        self.value_dense = Dense(32, activation='relu', name=\"value_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_value = Dropout(0.4, name=\"dropout_value_layer\")  # Dropout na rede de valor\n",
    "        self.value_output = Dense(1, activation='linear', name=\"value_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Rede para Vantagem (A)\n",
    "        self.advantage_dense = Dense(32, activation='relu', name=\"advantage_dense\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dropout_advantage = Dropout(0.4, name=\"dropout_advantage_layer\")  # Dropout na rede de vantagem\n",
    "        self.advantage_output = Dense(num_actions, activation='linear', name=\"advantage_output\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        # Camada convolucional 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu1\")\n",
    "        x = self.dropout_conv1(x, training=training)  # Dropout aplicado\n",
    "\n",
    "        # Camada convolucional 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x, name=\"relu2\")\n",
    "        x = self.dropout_conv2(x, training=training)  # Dropout aplicado\n",
    "        \n",
    "        # Atenção espacial\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout_flatten(x, training=training)  # Dropout aplicado após Flatten\n",
    "\n",
    "        # Camadas compartilhadas\n",
    "        x = self.dense_shared1(x)\n",
    "        x = self.dropout_shared(x, training=training)  # Dropout aplicado na camada compartilhada\n",
    "\n",
    "        # Valor (V)\n",
    "        v = self.value_dense(x)\n",
    "        v = self.dropout_value(v, training=training)  # Dropout aplicado na rede de valor\n",
    "        v = self.value_output(v)\n",
    "\n",
    "        # Vantagem (A)\n",
    "        a = self.advantage_dense(x)\n",
    "        a = self.dropout_advantage(a, training=training)  # Dropout aplicado na rede de vantagem\n",
    "        a = self.advantage_output(a)\n",
    "\n",
    "        # Combina V e A para calcular Q\n",
    "        q = v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return q\n",
    "    \n",
    "    def training_step(self, batch_data):\n",
    "        states, actions, targetQ = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "            loss = self.loss_fn(targetQ, Q)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        Q_values = self(state)\n",
    "        return tf.argmax(Q_values, axis=1)[0].numpy()  # Retorna a ação com o maior valor Q como um número Python\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        self.save(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        model_loaded = tf.keras.models.load_model(file_path)\n",
    "        return model_loaded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: nn_radar_double\n",
    "# ==========================================\n",
    "\n",
    "class nn_radar_double(tf.keras.Model):\n",
    "    def __init__(self, num_actions=9, input_shape=(7, 7, 3), discount_factor=0.99):\n",
    "        super(nn_radar_double, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Otimizador e função de perda\n",
    "        self.optimizer = Adam(learning_rate=lr)\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "        # Camada de entrada e processamento inicial de cores\n",
    "        self.input_layer = InputLayer(input_shape=input_shape, name=\"input_layer\")\n",
    "        self.color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"color_comb_layer\")\n",
    "\n",
    "        # Camadas convolucionais da rede principal\n",
    "        self.conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        # Módulo de atenção espacial\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        # Camadas densas\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "        # Camadas da rede-alvo (adicionadas as mesmas modificações)\n",
    "        self.target_color_comb_layer = ColorCombDepthwiseConv2D(kernel_size=(7, 7), activation='relu', name=\"target_color_comb_layer\")\n",
    "        self.target_conv1 = Conv2D(32, (4, 4), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm1 = BatchNormalization()\n",
    "        self.target_relu1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation=None, padding=\"same\")\n",
    "        self.target_batch_norm2 = BatchNormalization()\n",
    "        self.target_relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.target_spatial_attention = SpatialAttentionModule(kernel_size=4)\n",
    "\n",
    "        self.target_flatten = Flatten()\n",
    "        self.target_dense1 = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_dense2 = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_regularization))\n",
    "        self.target_output_layer = Dense(num_actions, activation=\"linear\", kernel_regularizer=l2(l2_regularization))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede principal para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.color_comb_layer(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def target_call(self, inputs, training=False):\n",
    "        \"\"\"Chama a rede-alvo para inferência.\"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.target_color_comb_layer(x)\n",
    "\n",
    "        x = self.target_conv1(x)\n",
    "        x = self.target_batch_norm1(x, training=training)\n",
    "        x = self.target_relu1(x)\n",
    "\n",
    "        x = self.target_conv2(x)\n",
    "        x = self.target_batch_norm2(x, training=training)\n",
    "        x = self.target_relu2(x)\n",
    "\n",
    "        x = self.target_spatial_attention(x)\n",
    "        x = self.target_flatten(x)\n",
    "        x = self.target_dense1(x)\n",
    "\n",
    "        x = self.target_dense2(x)\n",
    "        return self.target_output_layer(x)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Sincroniza os pesos da rede principal para a rede-alvo.\"\"\"\n",
    "        self.target_color_comb_layer.set_weights(self.color_comb_layer.get_weights())\n",
    "        self.target_conv1.set_weights(self.conv1.get_weights())\n",
    "        self.target_batch_norm1.set_weights(self.batch_norm1.get_weights())\n",
    "        self.target_conv2.set_weights(self.conv2.get_weights())\n",
    "        self.target_batch_norm2.set_weights(self.batch_norm2.get_weights())\n",
    "        self.target_dense1.set_weights(self.dense1.get_weights())\n",
    "        self.target_dense2.set_weights(self.dense2.get_weights())\n",
    "        self.target_output_layer.set_weights(self.output_layer.get_weights())\n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "        \"\"\"Realiza uma etapa de treinamento com Double DQN.\"\"\"\n",
    "        states, actions, targetQ = batch_data\n",
    "        next_states, rewards, dones = targetQ\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predições da rede principal\n",
    "            Q_values = self(states, training=True)\n",
    "            actions_onehot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            Q = tf.reduce_sum(Q_values * actions_onehot, axis=1)\n",
    "\n",
    "            # Double DQN: Calcula o valor-alvo\n",
    "            main_Q_values_next = self(next_states)  # Rede principal para selecionar a melhor ação\n",
    "            next_actions = tf.argmax(main_Q_values_next, axis=1)\n",
    "            target_Q_values_next = self.target_call(next_states)  # Rede-alvo para calcular Q\n",
    "            target_Q = tf.reduce_sum(target_Q_values_next * tf.one_hot(next_actions, self.num_actions), axis=1)\n",
    "            target_Q = rewards + (1 - dones) * self.discount_factor * target_Q\n",
    "\n",
    "            # Calcula o loss\n",
    "            loss = self.loss_fn(target_Q, Q)\n",
    "\n",
    "        # Gradientes e atualização dos pesos\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Prediz a melhor ação para um dado estado.\"\"\"\n",
    "        Q_values = self(tf.expand_dims(state, axis=0))\n",
    "        return tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"Salva os pesos do modelo principal.\"\"\"\n",
    "        self.save_weights(file_path)\n",
    "        print(f\"Modelo salvo em: {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"Carrega os pesos do modelo principal.\"\"\"\n",
    "        self.load_weights(file_path)\n",
    "        print(f\"Modelo carregado de: {file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSessionDoubleDQN:\n",
    "    def __init__(self, task_name, env, model_nn_predator, model_nn_prey,\n",
    "                 num_episodes=1000, num_steps=10, buffer_capacity=1000, batch_size=32):\n",
    "        self.task_name = task_name\n",
    "        self.directory = f\"C:/Users/beLIVE/IA/DECISYS/final-models/train\"\n",
    "        self.env = env\n",
    "        self.model_nn_predator = model_nn_predator\n",
    "        self.model_nn_prey = model_nn_prey\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.replay_buffer_predators = deque(maxlen=buffer_capacity)\n",
    "        self.replay_buffer_preys = deque(maxlen=buffer_capacity)\n",
    "\n",
    "        # Configuração de epsilon\n",
    "        self.epsilon_initial = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_initial  # Inicializa com o valor inicial\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.batch_size = batch_size\n",
    "        self.save_interval = 250\n",
    "\n",
    "    def update_epsilon_old(self, episode):\n",
    "        \"\"\"\n",
    "        Atualiza o valor de epsilon com decaimento exponencial suave ao longo de todo o treinamento.\n",
    "        \"\"\"\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_initial - self.epsilon_min) * np.exp(-self.epsilon_decay_rate * episode)\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Atualiza o valor de epsilon com decaimento exponencial suave ao longo de todo o treinamento.\n",
    "        \"\"\"\n",
    "        epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_initial - self.epsilon_min) * np.exp(-epsilon_decay_rate * episode)\n",
    "\n",
    "    def train_model_old(self, buffer, model):\n",
    "        \"\"\"\n",
    "        Treina o modelo com um minibatch do buffer de replay, usando Double DQN.\n",
    "        \"\"\"\n",
    "        if len(buffer) < self.batch_size:\n",
    "            return None\n",
    "        minibatch = random.sample(buffer, self.batch_size)\n",
    "\n",
    "        loss_total = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_expanded = np.expand_dims(state, axis=0)\n",
    "            next_state_expanded = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "            # Formatação para o método `training_step`\n",
    "            batch_data = (state_expanded, [action], (next_state_expanded, reward, done))\n",
    "            loss = model.training_step(batch_data)\n",
    "            loss_total.append(loss.numpy().item())\n",
    "\n",
    "        return f\"Mean Loss: {np.mean(loss_total):.4f}\"\n",
    "    \n",
    "\n",
    "    def train_model(self, buffer, model):\n",
    "        \"\"\"\n",
    "        Treina o modelo com um minibatch do buffer de replay, usando Double DQN.\n",
    "        \"\"\"\n",
    "        if len(buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Amostra um minibatch\n",
    "        minibatch = random.sample(buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        # Formata os dados para o modelo\n",
    "        batch_data = (states, actions, (next_states, rewards, dones))\n",
    "        loss = model.training_step(batch_data)\n",
    "        return f\"Mean Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executa o treinamento principal.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            self.env.reset()\n",
    "            self.update_epsilon(episode)\n",
    "            episode_rewards = {'predator': [], 'prey': []}\n",
    "            episode_losses = {'predator': [], 'prey': []}\n",
    "            episode_dones = {'predator': 0, 'prey': 0}\n",
    "\n",
    "            for step in range(self.num_steps):\n",
    "                if not any(a.breed == 2 and a.is_alive for a in self.env.agents) or not any(a.breed == 0 and a.is_alive for a in self.env.agents):\n",
    "                    break\n",
    "\n",
    "                for agent in self.env.agents:\n",
    "                    if not agent.is_alive:\n",
    "                        continue\n",
    "\n",
    "                    # Obtem o estado atual e seleciona a ação\n",
    "                    state = self.env.render_agent(agent)\n",
    "                    state_expanded = np.expand_dims(state, 0)\n",
    "                    model = self.model_nn_prey if agent.breed == 2 else self.model_nn_predator\n",
    "\n",
    "                    # Seleção de ação com epsilon-greedy\n",
    "                    if np.random.rand() > self.epsilon:\n",
    "                        Q_values = model(state_expanded)\n",
    "                        action = tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "                    else:\n",
    "                        action = np.random.randint(0, 9)\n",
    "\n",
    "                    # Executa a ação e coleta o feedback\n",
    "                    next_state, reward, done, feedback = agent.step(action)\n",
    "\n",
    "                    # Atualiza o buffer de replay\n",
    "                    buffer = self.replay_buffer_preys if agent.breed == 2 else self.replay_buffer_predators\n",
    "                    buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                    # Done status\n",
    "                    if done:\n",
    "                        agent.target = None\n",
    "                        agent.is_done = False\n",
    "                        agent.current_target = None\n",
    "                        self.env.agents = [a for a in self.env.agents if a.is_alive]\n",
    "\n",
    "                    # Atualiza métricas de recompensas\n",
    "                    if agent.breed == 0:  # Predador\n",
    "                        episode_rewards['predator'].append(reward)\n",
    "                        episode_dones['predator'] += 1 if done else 0\n",
    "                    elif agent.breed == 2:  # Presa\n",
    "                        episode_rewards['prey'].append(reward)\n",
    "                        episode_dones['prey'] += 1 if done else 0\n",
    "\n",
    "\n",
    "\n",
    "           # Treina os modelos ao final de cada episódio\n",
    "            loss_predators = self.train_model(self.replay_buffer_predators, self.model_nn_predator)\n",
    "            if loss_predators:\n",
    "                episode_losses['predator'].append(float(loss_predators.split(\":\")[1].strip()))\n",
    "            loss_preys = self.train_model(self.replay_buffer_preys, self.model_nn_prey)\n",
    "            if loss_preys:\n",
    "                episode_losses['prey'].append(float(loss_preys.split(\":\")[1].strip()))\n",
    "\n",
    "            # Logs e métricas\n",
    "            print(f\"Episode: {episode}, Predator Loss: {loss_predators}, Prey Loss: {loss_preys}, Epsilon: {self.epsilon:.4f}\")\n",
    "            \n",
    "            # Salva as métricas do episódio\n",
    "            self.save_to_file(episode, step, episode_rewards, episode_losses, self.epsilon)\n",
    "\n",
    "            # Sincronizar as redes-alvo a cada 10 episódios\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                self.model_nn_predator.update_target_network()\n",
    "                self.model_nn_prey.update_target_network()\n",
    "\n",
    "\n",
    "\n",
    "            # Salva os modelos periodicamente\n",
    "            if episode == 10 or (episode) % self.save_interval == 0:\n",
    "                file_path_model_prey = os.path.join(self.directory, f'model_{self.task_name}_prey_{episode}.h5')\n",
    "                file_path_model_predator = os.path.join(self.directory, f'model_{self.task_name}_predator_{episode}.h5')\n",
    "                self.model_nn_prey.save_model(file_path_model_prey)\n",
    "                self.model_nn_predator.save_model(file_path_model_predator)\n",
    "\n",
    "    def save_to_file(self, episode, step, episode_rewards, losses, epsilon):\n",
    "\n",
    "        stats = {}\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            rewards = episode_rewards[agent_type]\n",
    "            losses_values = losses[agent_type]\n",
    "\n",
    "            stats[agent_type] = {\n",
    "                'mean_reward': np.mean(rewards) if rewards else 0,\n",
    "                'median_reward': np.median(rewards) if rewards else 0,\n",
    "                'std_reward': np.sum(rewards) if rewards else 0,\n",
    "                'mean_loss': np.mean(losses_values) if losses_values else 0,\n",
    "                'median_loss': np.median(losses_values) if losses_values else 0,\n",
    "                'std_loss': np.sum(losses_values) if losses_values else 0\n",
    "            }\n",
    "        total_prey, total_predators, _ = self.env.population_count()\n",
    "        # Criação de arquivos separados\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            file_name = f\"{self.task_name}_{agent_type}.txt\"\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(\n",
    "                    f\"Episode: {episode}, step: {step}, Pop: {total_prey}/{total_predators}, \"\n",
    "                    f\"Epsilon: {epsilon:.2f}, \"\n",
    "                    f\"Mean Loss: {stats[agent_type]['mean_loss']:.2f}, Median Loss: {stats[agent_type]['median_loss']:.2f}, \"\n",
    "                    f\"Std Loss: {stats[agent_type]['std_loss']:.2f}, \"\n",
    "                    f\"Mean Reward: {stats[agent_type]['mean_reward']:.2f}, \"\n",
    "                    f\"Median Reward: {stats[agent_type]['median_reward']:.2f}, \"\n",
    "                    f\"Std Reward: {stats[agent_type]['std_reward']:.2f}\\n\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSessionPER:\n",
    "    def __init__(self, task_name, env, model_nn_predator, model_nn_prey, \n",
    "                 num_episodes=1000, num_steps=10, buffer_capacity=1000, batch_size=32, alpha=0.8, beta_start=0.4, beta_frames=5000):\n",
    "        #, alpha=0.6, beta_start=0.4, beta_frames=1000):\n",
    "        self.task_name = task_name\n",
    "        self.directory = f\"C:/Users/beLIVE/IA/DECISYS/final-models/train\"\n",
    "        self.env = env\n",
    "        self.model_nn_predator = model_nn_predator\n",
    "        self.model_nn_prey = model_nn_prey\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # Buffers de replay com prioridade\n",
    "        self.replay_buffer_predators = PrioritizedReplayBuffer(buffer_capacity, alpha)\n",
    "        self.replay_buffer_preys = PrioritizedReplayBuffer(buffer_capacity, alpha)\n",
    "\n",
    "        # Configuração de epsilon\n",
    "        self.epsilon_initial = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_initial\n",
    "\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Adicionando save_interval\n",
    "        self.save_interval = 250 # Intervalo para salvar os modelos\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"Aumenta beta linearmente ao longo dos frames.\"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Atualiza o valor de epsilon com decaimento exponencial suave ao longo de todo o treinamento.\n",
    "        \"\"\"\n",
    "        epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_initial - self.epsilon_min) * np.exp(-epsilon_decay_rate * episode)\n",
    "\n",
    "    def train_model(self, buffer, model, frame_idx):\n",
    "        if len(buffer.buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        # Obter beta ajustado para o frame atual\n",
    "        beta = self.beta_by_frame(frame_idx)\n",
    "\n",
    "        # Amostragem do buffer com prioridades\n",
    "        minibatch, indices, weights = buffer.sample(self.batch_size, beta)\n",
    "\n",
    "        # Processar minibatch em tensores para eficiência\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
    "\n",
    "        # Predições para os próximos estados (vetorizado)\n",
    "        next_q_values = model(next_states, training=False)  # Mantém cálculo em TensorFlow\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "\n",
    "        # Calcular TD Targets diretamente em TensorFlow\n",
    "        td_targets = rewards + (1 - dones) * model.discount_factor * max_next_q_values\n",
    "\n",
    "        # Predições para os estados atuais\n",
    "        current_q_values = model(states, training=False)\n",
    "        selected_q_values = tf.gather_nd(\n",
    "            current_q_values, \n",
    "            tf.stack([tf.range(self.batch_size), actions], axis=1)\n",
    "        )\n",
    "\n",
    "        # TD Errors calculados diretamente\n",
    "        td_errors = tf.abs(td_targets - selected_q_values)\n",
    "\n",
    "        # Pesos normalizados (opcional, para maior estabilidade)\n",
    "        normalized_weights = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
    "        if tf.reduce_sum(normalized_weights) > 0:\n",
    "            normalized_weights /= tf.reduce_sum(normalized_weights)\n",
    "\n",
    "        # Executa o treinamento do modelo\n",
    "        loss = model.training_step((states, actions, td_targets, normalized_weights))\n",
    "\n",
    "        # Atualizar prioridades no buffer de replay\n",
    "        buffer.update_priorities(indices, td_errors.numpy().tolist())\n",
    "\n",
    "        return f\"Mean Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "    def train_model_old(self, buffer, model, frame_idx):\n",
    "        if len(buffer.buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        beta = self.beta_by_frame(frame_idx)\n",
    "        minibatch, indices, weights = buffer.sample(self.batch_size, beta)\n",
    "\n",
    "        # Transformar minibatch para processamento vetorizado\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
    "        \n",
    "        # Predições para o próximo estado\n",
    "        next_q_values = model(next_states).numpy()\n",
    "        max_next_q_values = np.max(next_q_values, axis=1)\n",
    "        \n",
    "        # Calcula TD Target\n",
    "        td_targets = rewards + (1 - dones) * model.discount_factor * max_next_q_values\n",
    "\n",
    "        # Predições para o estado atual\n",
    "        current_q_values = model(states).numpy()\n",
    "        td_errors = np.abs(td_targets - current_q_values[np.arange(self.batch_size), actions])\n",
    "\n",
    "        # Normaliza os pesos\n",
    "        weights = weights / np.sum(weights) if np.sum(weights) > 0 else weights\n",
    "        loss = model.training_step((states, actions, td_targets, weights))\n",
    "\n",
    "        # Atualiza as prioridades no buffer\n",
    "        buffer.update_priorities(indices, td_errors.tolist())\n",
    "\n",
    "        return f\"Mean Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "    def train_model_old(self, buffer, model, frame_idx):\n",
    "        if len(buffer.buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        beta = self.beta_by_frame(frame_idx)\n",
    "        minibatch, indices, weights = buffer.sample(self.batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
    "        \n",
    "        next_q_values = model(next_states).numpy()\n",
    "        max_next_q_values = np.max(next_q_values, axis=1)\n",
    "        td_targets = rewards + (1 - dones) * model.discount_factor * max_next_q_values\n",
    "\n",
    "        current_q_values = model(states).numpy()\n",
    "        td_errors = np.abs(td_targets - current_q_values[np.arange(self.batch_size), actions])\n",
    "\n",
    "        weights = weights / np.max(weights) if np.max(weights) > 0 else weights\n",
    "        loss = model.training_step((states, actions, td_targets, weights))\n",
    "        buffer.update_priorities(indices, td_errors.tolist())\n",
    "\n",
    "        return f\"Mean Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Executa o treinamento principal.\"\"\"\n",
    "        frame_idx = 0\n",
    "        for episode in range(self.num_episodes):\n",
    "            self.env.reset()\n",
    "            self.update_epsilon(episode)\n",
    "            episode_rewards = {'predator': [], 'prey': []}\n",
    "            episode_losses = {'predator': [], 'prey': []}\n",
    "            episode_dones = {'predator': 0, 'prey': 0}\n",
    "\n",
    "            for step in range(self.num_steps):\n",
    "                if not any(a.breed == 2 and a.is_alive for a in self.env.agents) or not any(a.breed == 0 and a.is_alive for a in self.env.agents):\n",
    "                    break\n",
    "\n",
    "                for agent in self.env.agents:\n",
    "                    if not agent.is_alive:\n",
    "                        continue\n",
    "\n",
    "                    # Obtem o estado atual e seleciona a ação\n",
    "                    state = self.env.render_agent(agent)\n",
    "                    state_expanded = np.expand_dims(state, 0)\n",
    "                    model = self.model_nn_prey if agent.breed == 2 else self.model_nn_predator\n",
    "\n",
    "                    # Seleção de ação com epsilon-greedy\n",
    "                    if np.random.rand() > self.epsilon:\n",
    "                        Q_values = model(state_expanded)\n",
    "                        action = tf.argmax(Q_values, axis=1).numpy()[0]\n",
    "                    else:\n",
    "                        action = np.random.randint(0, 9)\n",
    "\n",
    "                    # Executa a ação e coleta o feedback\n",
    "                    next_state, reward, done, feedback = agent.step(action)\n",
    "\n",
    "                    # Atualiza o buffer de replay\n",
    "                    buffer = self.replay_buffer_preys if agent.breed == 2 else self.replay_buffer_predators\n",
    "                    td_error = abs(reward + (1 - done) * model.discount_factor * np.max(model(np.expand_dims(next_state, axis=0))) - model(state_expanded)[0, action])\n",
    "                    buffer.add((state, action, reward, next_state, done), td_error)\n",
    "\n",
    "\n",
    "                    # Done status\n",
    "                    if done:\n",
    "                        agent.target = None\n",
    "                        agent.is_done = False\n",
    "                        agent.current_target = None\n",
    "                        self.env.agents = [a for a in self.env.agents if a.is_alive]\n",
    "\n",
    "                    # Atualiza métricas\n",
    "                    if agent.breed == 0:  # Predador\n",
    "                        episode_rewards['predator'].append(reward)\n",
    "                        episode_dones['predator'] += 1 if done else 0\n",
    "                    elif agent.breed == 2:  # Presa\n",
    "                        episode_rewards['prey'].append(reward)\n",
    "                        episode_dones['prey'] += 1 if done else 0\n",
    "\n",
    "                    # Exibe detalhes do agente\n",
    "                    pop_prey, pop_predator, _ = self.env.population_count()\n",
    "                    print(f\"{episode}/{step}: {agent.name}, Pop: {pop_prey}/{pop_predator}, \"\n",
    "                        f\"Position: ({agent.x}, {agent.y}), Action: {action}, Reward: {reward}, Done: {done}, Feedback: {feedback}\")\n",
    "\n",
    "\n",
    "                frame_idx += 1\n",
    "\n",
    "            # Treina os modelos ao final de cada episódio\n",
    "            loss_predators = self.train_model(self.replay_buffer_predators, self.model_nn_predator, frame_idx)\n",
    "            if loss_predators:\n",
    "                episode_losses['predator'].append(float(loss_predators.split(\":\")[1].strip()))\n",
    "            loss_preys = self.train_model(self.replay_buffer_preys, self.model_nn_prey, frame_idx)\n",
    "            if loss_preys:\n",
    "                episode_losses['prey'].append(float(loss_preys.split(\":\")[1].strip()))\n",
    "\n",
    "             # Salva as métricas do episódio\n",
    "            self.save_to_file(episode, step, episode_rewards, episode_losses, self.epsilon)\n",
    "\n",
    "            # Salva os modelos periodicamente\n",
    "            if episode == 10 or (episode) % self.save_interval == 0:\n",
    "                file_path_model_prey = os.path.join(self.directory, f'model_{self.task_name}_prey_{episode}.h5')\n",
    "                file_path_model_predator = os.path.join(self.directory, f'model_{self.task_name}_predator_{episode}.h5')\n",
    "                try:\n",
    "                    self.model_nn_prey.save_weights(file_path_model_prey)\n",
    "                    print(f\"Pesos do modelo de presa salvos com sucesso em {file_path_model_prey}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ocorreu um erro ao salvar os pesos do modelo de presa: {e}\")\n",
    "                try:\n",
    "                    self.model_nn_predator.save_weights(file_path_model_predator)\n",
    "                    print(f\"Pesos do modelo de predador salvos com sucesso em {file_path_model_predator}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ocorreu um erro ao salvar os pesos do modelo de predador: {e}\")\n",
    "\n",
    "    def save_to_file(self, episode, step, episode_rewards, losses, epsilon):\n",
    "\n",
    "        stats = {}\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            rewards = episode_rewards[agent_type]\n",
    "            losses_values = losses[agent_type]\n",
    "\n",
    "            stats[agent_type] = {\n",
    "                'mean_reward': np.mean(rewards) if rewards else 0,\n",
    "                'median_reward': np.median(rewards) if rewards else 0,\n",
    "                'std_reward': np.sum(rewards) if rewards else 0,\n",
    "                'mean_loss': np.mean(losses_values) if losses_values else 0,\n",
    "                'median_loss': np.median(losses_values) if losses_values else 0,\n",
    "                'std_loss': np.sum(losses_values) if losses_values else 0\n",
    "            }\n",
    "        total_prey, total_predators, _ = self.env.population_count()\n",
    "        # Criação de arquivos separados\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            file_name = f\"{self.task_name}_{agent_type}.txt\"\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(\n",
    "                    f\"Episode: {episode}, step: {step}, Pop: {total_prey}/{total_predators}, \"\n",
    "                    f\"Epsilon: {epsilon:.2f}, \"\n",
    "                    f\"Mean Loss: {stats[agent_type]['mean_loss']:.2f}, Median Loss: {stats[agent_type]['median_loss']:.2f}, \"\n",
    "                    f\"Std Loss: {stats[agent_type]['std_loss']:.2f}, \"\n",
    "                    f\"Mean Reward: {stats[agent_type]['mean_reward']:.2f}, \"\n",
    "                    f\"Median Reward: {stats[agent_type]['median_reward']:.2f}, \"\n",
    "                    f\"Std Reward: {stats[agent_type]['std_reward']:.2f}\\n\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession:\n",
    "    def __init__(self, task_name, env, model_nn_predator, model_nn_prey, num_episodes=1000, num_steps=10, buffer_capacity=1000, batch_size=32):\n",
    "\n",
    "        self.task_name = task_name\n",
    "        self.directory = f\"C:/Users/beLIVE/IA/DECISYS/final-models/train\"\n",
    "        self.env = env\n",
    "        self.model_nn_predator = model_nn_predator\n",
    "        self.model_nn_prey = model_nn_prey\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.replay_buffer_predators = deque(maxlen=buffer_capacity)\n",
    "        self.replay_buffer_preys = deque(maxlen=buffer_capacity)\n",
    "\n",
    "        # Configuração de epsilon\n",
    "        self.epsilon_initial = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.half_decay_period = num_episodes // 2\n",
    "        #self.decay_constant = self.half_decay_period / np.log(self.epsilon_initial / self.epsilon_min)\n",
    "        self.epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_initial  # Inicializa com o valor inicial\n",
    "\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.batch_size = batch_size\n",
    "        self.save_interval = 250\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Atualiza o valor de epsilon com decaimento exponencial suave ao longo de todo o treinamento.\n",
    "        \"\"\"\n",
    "        epsilon_decay_rate = np.log(self.epsilon_initial / self.epsilon_min) / self.num_episodes\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_initial - self.epsilon_min) * np.exp(-epsilon_decay_rate * episode)\n",
    "\n",
    "\n",
    "    \n",
    "    def update_epsilon_old(self, episode):\n",
    "        \"\"\"\n",
    "        Atualiza o valor de epsilon para exploração baseado no episódio atual.\n",
    "        O epsilon decai exponencialmente até a metade do período total de episódios.\n",
    "        Após isso, mantém o valor mínimo.\n",
    "        \"\"\"\n",
    "        if episode < self.half_decay_period:\n",
    "            self.epsilon = self.epsilon_min + (self.epsilon_initial - self.epsilon_min) * np.exp(-episode / self.decay_constant)\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "    def train_model(self, buffer, model):\n",
    "        \"\"\"\n",
    "        Treina o modelo com um minibatch completo do buffer de replay.\n",
    "        \"\"\"\n",
    "        if len(buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        # Amostragem eficiente do buffer\n",
    "        indices = np.random.choice(len(buffer), self.batch_size, replace=False)\n",
    "        minibatch = [buffer[i] for i in indices]\n",
    "\n",
    "        # Separação das transições\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = tf.convert_to_tensor(np.array(states, dtype=np.float32))\n",
    "        actions = tf.convert_to_tensor(np.array(actions, dtype=np.int32))\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards, dtype=np.float32))\n",
    "        next_states = tf.convert_to_tensor(np.array(next_states, dtype=np.float32))\n",
    "        dones = tf.convert_to_tensor(np.array(dones, dtype=np.float32))\n",
    "\n",
    "        # Predições e cálculo vetorizado com TensorFlow\n",
    "        current_q_values = model(states)\n",
    "        next_q_values = model(next_states)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "\n",
    "        # Cálculo dos valores-alvo\n",
    "        targetQ = rewards + (1 - dones) * self.discount_factor * max_next_q_values\n",
    "\n",
    "        # Treinamento\n",
    "        batch_data = (states, actions, targetQ)\n",
    "        loss = model.training_step(batch_data)\n",
    "        return f\"Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_model_new_old(self, buffer, model):\n",
    "        \"\"\"\n",
    "        Treina o modelo com um minibatch completo do buffer de replay.\n",
    "        \"\"\"\n",
    "        if len(buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        # Amostragem do buffer\n",
    "        minibatch = random.sample(buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
    "\n",
    "        # Predições dos Q-values\n",
    "        current_q_values = model(states).numpy()\n",
    "        next_q_values = model(next_states).numpy()\n",
    "\n",
    "        # Cálculo vetorizado dos valores-alvo\n",
    "        max_next_q_values = np.max(next_q_values, axis=1)\n",
    "        targetQ = rewards + (1 - dones) * self.discount_factor * max_next_q_values\n",
    "\n",
    "        # Treinamento\n",
    "        batch_data = (states, actions, targetQ)\n",
    "        loss = model.training_step(batch_data)\n",
    "        return f\"Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "    def train_model_old(self, buffer, model):\n",
    "        \"\"\"\n",
    "        Treina o modelo com um minibatch completo do buffer de replay.\n",
    "        \"\"\"\n",
    "        # Certifique-se de que há dados suficientes no buffer\n",
    "        if len(buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        # Amostra um minibatch do replay buffer\n",
    "        minibatch = random.sample(buffer, self.batch_size)\n",
    "\n",
    "        # Separando os componentes do minibatch\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Converte os componentes para numpy arrays\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "\n",
    "        # Predição dos Q-values atuais e futuros\n",
    "        current_q_values = model(states, training=False).numpy()\n",
    "        next_q_values = model(next_states, training=False).numpy()\n",
    "\n",
    "        # Calcula os valores-alvo (targetQ) para as ações tomadas\n",
    "        targetQ = np.zeros(self.batch_size)  # Vetor 1D para os valores-alvo\n",
    "        for i in range(self.batch_size):\n",
    "            target_q_value = rewards[i]\n",
    "            if not dones[i]:  # Adiciona o valor descontado do próximo estado se o episódio não terminou\n",
    "                target_q_value += self.discount_factor * np.max(next_q_values[i])\n",
    "            targetQ[i] = target_q_value  # Define o valor-alvo para a ação tomada\n",
    "\n",
    "        # Realiza uma etapa de treinamento com o minibatch completo\n",
    "        batch_data = (states, actions, targetQ)\n",
    "        loss = model.training_step(batch_data)\n",
    "\n",
    "        return f\"Loss: {loss.numpy():.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executa o treinamento principal.\n",
    "        \"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            self.env.reset()\n",
    "            self.update_epsilon(episode)\n",
    "            episode_rewards = {'predator': [], 'prey': []}\n",
    "            episode_losses = {'predator': [], 'prey': []}\n",
    "            episode_dones = {'predator': 0, 'prey': 0}\n",
    "\n",
    "            for step in range(self.num_steps):\n",
    "                # Verifica se ainda há predadores e presas vivos\n",
    "                if not any(a.breed == 2 and a.is_alive for a in self.env.agents) or not any(a.breed == 0 and a.is_alive for a in self.env.agents):\n",
    "                    break\n",
    "\n",
    "                for agent in self.env.agents:\n",
    "                    if not agent.is_alive:\n",
    "                        continue\n",
    "\n",
    "                    # Obtem o estado atual e seleciona a ação\n",
    "                    state = self.env.render_agent(agent)\n",
    "                    state_expanded = np.expand_dims(state, 0)\n",
    "                    model = self.model_nn_prey if agent.breed == 2 else self.model_nn_predator\n",
    "\n",
    "                    # Seleção de ação com epsilon-greedy\n",
    "                    if np.random.rand() > self.epsilon:\n",
    "                        Q_values = model.predict(state_expanded, verbose=0)\n",
    "                        action = np.argmax(Q_values[0])\n",
    "                    else:\n",
    "                        action = np.random.randint(0, 9)\n",
    "\n",
    "                    # Executa a ação e coleta o feedback\n",
    "                    next_state, reward, done, feedback = agent.step(action)\n",
    "\n",
    "                    \n",
    "\n",
    "                    # Atualiza o buffer de replay\n",
    "                    buffer = self.replay_buffer_preys if agent.breed == 2 else self.replay_buffer_predators\n",
    "                    buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    # Done status\n",
    "                    if done:\n",
    "                        agent.target = None\n",
    "                        agent.is_done = False\n",
    "                        agent.current_target = None\n",
    "                        self.env.agents = [a for a in self.env.agents if a.is_alive]\n",
    "\n",
    "                    # Atualiza métricas de recompensas e estados terminais\n",
    "                    if agent.breed == 0:  # Predador\n",
    "                        episode_rewards['predator'].append(reward)\n",
    "                        episode_dones['predator'] += 1 if done else 0\n",
    "                    elif agent.breed == 2:  # Presa\n",
    "                        episode_rewards['prey'].append(reward)\n",
    "                        episode_dones['prey'] += 1 if done else 0\n",
    "\n",
    "                    # Exibe detalhes do agente\n",
    "                    pop_prey, pop_predator, _ = self.env.population_count()\n",
    "                    print(f\"{episode}/{step}: {agent.name}, Pop: {pop_prey}/{pop_predator}, \"\n",
    "                        f\"Position: ({agent.x}, {agent.y}), Action: {action}, Reward: {reward}, Done: {done}, Feedback: {feedback}\")\n",
    "\n",
    "\n",
    "\n",
    "            # Treina os modelos no final do episódio\n",
    "            loss_predators = self.train_model(self.replay_buffer_predators, self.model_nn_predator)\n",
    "            if loss_predators:\n",
    "                episode_losses['predator'].append(float(loss_predators.split(\":\")[1].strip()))\n",
    "            loss_preys = self.train_model(self.replay_buffer_preys, self.model_nn_prey)\n",
    "            if loss_preys:\n",
    "                episode_losses['prey'].append(float(loss_preys.split(\":\")[1].strip()))\n",
    "\n",
    "            # Salva as métricas do episódio\n",
    "            self.save_to_file(episode, step, episode_rewards, episode_losses, self.epsilon)\n",
    "\n",
    "            # Salva os modelos periodicamente\n",
    "            if episode == 10 or (episode) % self.save_interval == 0:\n",
    "\n",
    "                # Nome do arquivo para salvar os pesos do modelo\n",
    "                file_path_model_prey = os.path.join(self.directory, f'model_{self.task_name}_prey_{episode}.h5')\n",
    "                # Nome do arquivo para salvar os pesos do modelo\n",
    "                file_path_model_predator = os.path.join(self.directory, f'model_{self.task_name}_predator_{episode}.h5')\n",
    "                try:\n",
    "                    self.model_nn_prey.save_weights(file_path_model_prey)\n",
    "                    print(f\"Pesos do modelo de presa salvos com sucesso em {file_path_model_prey}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ocorreu um erro ao salvar os pesos do modelo de presa: {e}\")\n",
    "                try:\n",
    "                    self.model_nn_predator.save_weights(file_path_model_predator)\n",
    "                    print(f\"Pesos do modelo de presa salvos com sucesso em {file_path_model_predator}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Ocorreu um erro ao salvar os pesos do modelo de presa: {e}\")\n",
    "\n",
    "\n",
    "    def save_to_file(self, episode, step, episode_rewards, losses, epsilon):\n",
    "\n",
    "        stats = {}\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            rewards = episode_rewards[agent_type]\n",
    "            losses_values = losses[agent_type]\n",
    "\n",
    "            stats[agent_type] = {\n",
    "                'mean_reward': np.mean(rewards) if rewards else 0,\n",
    "                'median_reward': np.median(rewards) if rewards else 0,\n",
    "                'std_reward': np.sum(rewards) if rewards else 0,\n",
    "                'mean_loss': np.mean(losses_values) if losses_values else 0,\n",
    "                'median_loss': np.median(losses_values) if losses_values else 0,\n",
    "                'std_loss': np.sum(losses_values) if losses_values else 0\n",
    "            }\n",
    "        total_prey, total_predators, _ = self.env.population_count()\n",
    "        # Criação de arquivos separados\n",
    "        for agent_type in ['predator', 'prey']:\n",
    "            file_name = f\"{self.task_name}_{agent_type}.txt\"\n",
    "            with open(file_name, \"a\") as file:\n",
    "                file.write(\n",
    "                    f\"Episode: {episode}, step: {step}, Pop: {total_prey}/{total_predators}, \"\n",
    "                    f\"Epsilon: {epsilon:.2f}, \"\n",
    "                    f\"Mean Loss: {stats[agent_type]['mean_loss']:.2f}, Median Loss: {stats[agent_type]['median_loss']:.2f}, \"\n",
    "                    f\"Std Loss: {stats[agent_type]['std_loss']:.2f}, \"\n",
    "                    f\"Mean Reward: {stats[agent_type]['mean_reward']:.2f}, \"\n",
    "                    f\"Median Reward: {stats[agent_type]['median_reward']:.2f}, \"\n",
    "                    f\"Std Reward: {stats[agent_type]['std_reward']:.2f}\\n\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1200\n",
    "buffer = 1000\n",
    "batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVAÇÃO: training-radar-dqn-double\n",
    "# ==========================================\n",
    "\n",
    "# Definindo o nome da tarefa e o log\n",
    "task_name = \"nn_dqn_double64\"\n",
    "input_shape = (7, 7, 3)\n",
    "size = 10\n",
    "\n",
    "# Criando os modelos Double DQN para predadores e presas\n",
    "model_nn_predator = nn_dqn_double(num_actions=9, input_shape=input_shape)\n",
    "model_nn_prey = nn_dqn_double(num_actions=9, input_shape=input_shape)\n",
    "\n",
    "# Sincroniza os pesos com a rede-alvo\n",
    "model_nn_predator.update_target_network()\n",
    "model_nn_prey.update_target_network()\n",
    "\n",
    "# Criando o ambiente\n",
    "env = Env(sizeX=10, sizeY=10, ray=3)\n",
    "\n",
    "# Definindo o tamanho do ambiente e número de episódios e passos\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "num_preys = 10\n",
    "num_predators = 5\n",
    "\n",
    "num_obstacle = int(env.sizeX * env.sizeX * 0.10)\n",
    "\n",
    "# Povoando o ambiente com obstáculos\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "\n",
    "# Criando a sessão de treinamento\n",
    "training_session = TrainingSessionDoubleDQN(\n",
    "    task_name=task_name,\n",
    "    env=env,\n",
    "    model_nn_predator=model_nn_predator,\n",
    "    model_nn_prey=model_nn_prey,\n",
    "    num_episodes=num_episodes,\n",
    "    num_steps=num_steps,\n",
    "    buffer_capacity=buffer, \n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "# Executando o treinamento\n",
    "training_session.run()\n",
    "\n",
    "directory = f\"C:/Users/beLIVE/IA/DECISYS/final-models/train\"\n",
    "file_path_model_prey = os.path.join(directory, f'model_{task_name}_prey_1000.h5')\n",
    "file_path_model_predator = os.path.join(directory, f'model_{task_name}_predator_1000.h5')\n",
    "model_nn_prey.save_model(file_path_model_prey)\n",
    "model_nn_predator.save_model(file_path_model_predator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVAÇÃO: training-radar-dqn-per\n",
    "# ==========================================\n",
    "\n",
    "# Definindo o nome da tarefa e o log\n",
    "task_name = \"nn_dqn_per64\"\n",
    "input_shape = (7, 7, 3)\n",
    "size = 10\n",
    "\n",
    "# Modelo Neural utilizado\n",
    "model_nn_predator = nn_dqn_per()\n",
    "model_nn_prey = nn_dqn_per()\n",
    "\n",
    "model_nn_predator.build(input_shape=(None,) + input_shape)\n",
    "model_nn_prey.build(input_shape=(None,) + input_shape)\n",
    "\n",
    "# Criando o ambiente\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "\n",
    "# Definindo o tamanho do ambiente e número de episódios e passos\n",
    "num_steps = 10\n",
    "\n",
    "num_preys = 10\n",
    "num_predators = 5\n",
    "\n",
    "num_obstacle = int(env.sizeX * env.sizeX * 0.10)\n",
    "\n",
    "# Povoando o ambiente com obstáculos\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "\n",
    "training_session = TrainingSessionPER(\n",
    "    task_name=task_name,\n",
    "    env=env,\n",
    "    model_nn_predator=model_nn_predator,\n",
    "    model_nn_prey=model_nn_prey,\n",
    "    num_episodes=num_episodes,       # 5000 episódios\n",
    "    num_steps=10,\n",
    "    buffer_capacity=buffer,   # Buffer maior para treinos longos\n",
    "    batch_size=batch,           # Tamanho do minibatch\n",
    "    alpha=0.3,               # Menor impacto de TD-Errors extremos\n",
    "    beta_start=0.4,          # Beta inicial\n",
    " ) \n",
    "\n",
    "# Executando o treinamento\n",
    "training_session.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVAÇÃO: training-radar-dqn-dueling\n",
    "# ==========================================\n",
    "\n",
    "# Definindo o nome da tarefa e o log\n",
    "task_name = \"nn_dqn_dueling64\"\n",
    "input_shape = (7, 7, 3)\n",
    "size = 10\n",
    "\n",
    "# Modelo Neural utilizado\n",
    "model_nn_predator = nn_dqn_dueling()\n",
    "model_nn_prey = nn_dqn_dueling()\n",
    "\n",
    "model_nn_predator.build(input_shape=(None,) + input_shape)\n",
    "model_nn_prey.build(input_shape=(None,) + input_shape)\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "\n",
    "# Definindo o tamanho do ambiente e número de episódios e passos\n",
    "num_steps = 10\n",
    "\n",
    "num_preys = 10\n",
    "num_predators = 5\n",
    "\n",
    "num_obstacle = int(env.sizeX * env.sizeX * 0.10)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "\n",
    "# Criando a sessão de treinamento\n",
    "training_session = TrainingSession(task_name, env, model_nn_predator, model_nn_prey, num_episodes, num_steps, buffer_capacity=buffer, batch_size=batch)\n",
    "\n",
    "# Executando o treinamento\n",
    "training_session.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVAÇÃO: training-radar-dqn\n",
    "# ==========================================\n",
    "\n",
    "# Definindo o nome da tarefa e o log\n",
    "task_name = \"nn_dqn64\"\n",
    "input_shape = (7, 7, 3)\n",
    "size = 10\n",
    "\n",
    "# Modelo Neural utilizado\n",
    "model_nn_predator = nn_dqn()\n",
    "model_nn_prey = nn_dqn()\n",
    "\n",
    "model_nn_predator.build(input_shape=(None,) + input_shape)\n",
    "model_nn_prey.build(input_shape=(None,) + input_shape)\n",
    "\n",
    "env = Env(sizeX=size, sizeY=size, ray=3)\n",
    "\n",
    "# Definindo o tamanho do ambiente e número de episódios e passos\n",
    "num_steps = 10\n",
    "\n",
    "num_preys = 10\n",
    "num_predators = 5\n",
    "\n",
    "num_obstacle = int(env.sizeX * env.sizeX * 0.10)\n",
    "\n",
    "# Povoando o ambiente com obstacle\n",
    "for o in range(num_obstacle):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        obstacle = Obstacle(x, y)\n",
    "        env.add_obstacle(obstacle)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o obstacle.\")\n",
    "\n",
    "# Povoando o ambiente com presas\n",
    "for j in range(num_preys):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        prey = Prey(x, y, env, j, model_nn_prey)\n",
    "        env.add_agent(prey)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o prey.\")\n",
    "\n",
    "# Povoando o ambiente com predadores\n",
    "for i in range(num_predators):\n",
    "    pos = env.new_position()\n",
    "    if pos is not None:\n",
    "        x, y = pos\n",
    "        predator = Predator(x, y, env, i, model_nn_predator)\n",
    "        env.add_agent(predator)\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma posição nova para o predator.\")\n",
    "\n",
    "# Criando a sessão de treinamento\n",
    "training_session = TrainingSession(task_name, env, model_nn_predator, model_nn_prey, num_episodes, num_steps, buffer_capacity=buffer, batch_size=batch)\n",
    "\n",
    "# Executando o treinamento\n",
    "training_session.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "p38-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
